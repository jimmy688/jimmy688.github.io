<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Kafka概述 | 个人博客 | JiiiiG</title>
    <meta name="generator" content="VuePress 1.8.0">
    
    <meta name="description" content="自是年少，韶华倾负">
    
    <link rel="preload" href="/assets/css/0.styles.420b7c1f.css" as="style"><link rel="preload" href="/assets/js/app.fdd7a502.js" as="script"><link rel="preload" href="/assets/js/16.375b747e.js" as="script"><link rel="preload" href="/assets/js/20.b3582e09.js" as="script"><link rel="prefetch" href="/assets/js/10.bbd38b9e.js"><link rel="prefetch" href="/assets/js/11.f49fc7d3.js"><link rel="prefetch" href="/assets/js/12.c5d97aa1.js"><link rel="prefetch" href="/assets/js/13.426d0224.js"><link rel="prefetch" href="/assets/js/14.1f257a58.js"><link rel="prefetch" href="/assets/js/15.be29bb6a.js"><link rel="prefetch" href="/assets/js/17.d02cdd8b.js"><link rel="prefetch" href="/assets/js/18.5d2371a4.js"><link rel="prefetch" href="/assets/js/19.038919a3.js"><link rel="prefetch" href="/assets/js/2.d6257727.js"><link rel="prefetch" href="/assets/js/21.0c1e826b.js"><link rel="prefetch" href="/assets/js/22.91bff1a5.js"><link rel="prefetch" href="/assets/js/23.faacea60.js"><link rel="prefetch" href="/assets/js/24.69816497.js"><link rel="prefetch" href="/assets/js/25.c9fcaa62.js"><link rel="prefetch" href="/assets/js/26.d28c3abe.js"><link rel="prefetch" href="/assets/js/27.1518da7d.js"><link rel="prefetch" href="/assets/js/28.748a5581.js"><link rel="prefetch" href="/assets/js/29.775019f1.js"><link rel="prefetch" href="/assets/js/3.141002a4.js"><link rel="prefetch" href="/assets/js/30.74e7c570.js"><link rel="prefetch" href="/assets/js/31.190b11f2.js"><link rel="prefetch" href="/assets/js/32.d8f4f116.js"><link rel="prefetch" href="/assets/js/33.c489bc81.js"><link rel="prefetch" href="/assets/js/34.db609d51.js"><link rel="prefetch" href="/assets/js/35.ad3402e6.js"><link rel="prefetch" href="/assets/js/36.45f11923.js"><link rel="prefetch" href="/assets/js/37.b47e0576.js"><link rel="prefetch" href="/assets/js/38.58b934a2.js"><link rel="prefetch" href="/assets/js/39.e4d6bcdf.js"><link rel="prefetch" href="/assets/js/4.e9b44388.js"><link rel="prefetch" href="/assets/js/40.555106bc.js"><link rel="prefetch" href="/assets/js/41.4387ceb1.js"><link rel="prefetch" href="/assets/js/42.9bd4aea0.js"><link rel="prefetch" href="/assets/js/43.587e7966.js"><link rel="prefetch" href="/assets/js/44.80285763.js"><link rel="prefetch" href="/assets/js/45.f978da77.js"><link rel="prefetch" href="/assets/js/46.d93e3f2e.js"><link rel="prefetch" href="/assets/js/47.c15bd2aa.js"><link rel="prefetch" href="/assets/js/48.cb55fa72.js"><link rel="prefetch" href="/assets/js/49.d0f4e454.js"><link rel="prefetch" href="/assets/js/5.b90b80cb.js"><link rel="prefetch" href="/assets/js/50.112e8260.js"><link rel="prefetch" href="/assets/js/51.a66bcd03.js"><link rel="prefetch" href="/assets/js/52.0c7cbfb1.js"><link rel="prefetch" href="/assets/js/53.67ef240b.js"><link rel="prefetch" href="/assets/js/54.2c2d082d.js"><link rel="prefetch" href="/assets/js/55.88f19a20.js"><link rel="prefetch" href="/assets/js/56.c726e920.js"><link rel="prefetch" href="/assets/js/57.eaee47c5.js"><link rel="prefetch" href="/assets/js/58.ac15a3ac.js"><link rel="prefetch" href="/assets/js/59.64bdc849.js"><link rel="prefetch" href="/assets/js/6.7eea74ff.js"><link rel="prefetch" href="/assets/js/7.a1370e98.js"><link rel="prefetch" href="/assets/js/8.2e898240.js"><link rel="prefetch" href="/assets/js/9.a2439994.js">
    <link rel="stylesheet" href="/assets/css/0.styles.420b7c1f.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">个人博客 | JiiiiG</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  主页
</a></div><div class="nav-item"><a href="/linux/linux常用命令/01linux常用命令01.html" class="nav-link">
  Linux
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="大数据" class="dropdown-title"><span class="title">大数据</span> <span class="arrow down"></span></button> <button type="button" aria-label="大数据" class="mobile-dropdown-title"><span class="title">大数据</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hadoop/01hadoop环境搭建/01搭建hadoop集群.html" class="nav-link">
  hadoop
</a></li><li class="dropdown-item"><!----> <a href="/hbase/hbase.html" class="nav-link">
  hbase
</a></li><li class="dropdown-item"><!----> <a href="/flink/flink.html" class="nav-link">
  flink
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="编程语言" class="dropdown-title"><span class="title">编程语言</span> <span class="arrow down"></span></button> <button type="button" aria-label="编程语言" class="mobile-dropdown-title"><span class="title">编程语言</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/kafka/.html" class="nav-link">
  java
</a></li></ul></div></div> <a href="https://github.com/MaLunan/press" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  主页
</a></div><div class="nav-item"><a href="/linux/linux常用命令/01linux常用命令01.html" class="nav-link">
  Linux
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="大数据" class="dropdown-title"><span class="title">大数据</span> <span class="arrow down"></span></button> <button type="button" aria-label="大数据" class="mobile-dropdown-title"><span class="title">大数据</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hadoop/01hadoop环境搭建/01搭建hadoop集群.html" class="nav-link">
  hadoop
</a></li><li class="dropdown-item"><!----> <a href="/hbase/hbase.html" class="nav-link">
  hbase
</a></li><li class="dropdown-item"><!----> <a href="/flink/flink.html" class="nav-link">
  flink
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="编程语言" class="dropdown-title"><span class="title">编程语言</span> <span class="arrow down"></span></button> <button type="button" aria-label="编程语言" class="mobile-dropdown-title"><span class="title">编程语言</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/kafka/.html" class="nav-link">
  java
</a></li></ul></div></div> <a href="https://github.com/MaLunan/press" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="kafka概述"><a href="#kafka概述" class="header-anchor">#</a> Kafka概述</h2> <h5 id="为什么有消息系统"><a href="#为什么有消息系统" class="header-anchor">#</a> 为什么有消息系统</h5> <p><strong>解耦</strong>
允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p> <p><strong>冗余</strong>
消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的&quot;插入-获取-删除&quot;范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。</p> <p><strong>扩展性</strong>
因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。</p> <p><strong>灵活性 &amp; 峰值处理能力</strong>
在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p> <p><strong>可恢复性</strong>
系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p> <p><strong>顺序保证</strong>
在大多使用场景下，数据处理的顺序都很重要。部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理大。（Kafka 保证一个 Partition 内的消息的有序性）</p> <p><strong>缓冲</strong>
有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p> <p><strong>异步通信</strong>
很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p> <blockquote><p>补充：</p> <p>解耦的通俗理解：解耦：假设生产者和消费者分别是两个类。如果让生产者直接调用消费者的某个方法，那么生产者对于消费者就会产生依赖（也就是耦合）。将来如果消费者的代码发生变化，可能会影响到生产者。而如果两者都依赖于某个缓冲区，两者之间不直接依赖，耦合也就相应降低了。生产者直接调用消费者的某个方法，还有另一个弊端。由于函数调用是同步的（或者叫阻塞的），在消费者的方法没有返回之前，生产者只好一直等在那边。万一消费者处理数据很慢，生产者就会白白糟蹋大好时光。缓冲区还有另一个好处。如果制造数据的速度时快时慢，缓冲区的好处就体现出来了。当数据制造快的时候，消费者来不及处理，未处理的数据可以暂时存在缓冲区中。等生产者的制造速度慢下来，消费者再慢慢处理掉。</p></blockquote> <h5 id="kafka核心概念"><a href="#kafka核心概念" class="header-anchor">#</a> Kafka核心概念</h5> <p>Kafka是最初由Linkedin公司开发，<strong>是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统）</strong>，常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。</p> <p>kafka是一个分布式消息队列。具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。Kafka就是一种发布-订阅模式。将消息保存在磁盘中，以顺序读写方式访问磁盘，避免随机读写导致性能瓶颈。</p> <h5 id="kafka特性"><a href="#kafka特性" class="header-anchor">#</a> kafka特性</h5> <p>高吞吐、低延迟</p> <blockquote><p>kakfa 最大的特点就是收发消息非常快，kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒。</p></blockquote> <p>高伸缩性</p> <blockquote><p>每个主题(topic) 包含多个分区(partition)，主题中的分区可以分布在不同的主机(broker)中。</p></blockquote> <p>持久性、可靠性</p> <blockquote><p>Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失。</p></blockquote> <p>容错性</p> <blockquote><p>允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作。</p></blockquote> <p>高并发</p> <blockquote><p>支持数千个客户端同时读写。</p></blockquote> <h2 id="kafka集群架构"><a href="#kafka集群架构" class="header-anchor">#</a> Kafka集群架构</h2> <p><img src="/assets/img/image-20200419215655315.cf8f803c.png" alt="image-20200419215655315"></p> <p>producer</p> <blockquote><p>消息生产者，发布消息到Kafka集群的终端或服务</p></blockquote> <p>broker</p> <blockquote><p>Kafka集群中包含的服务器，==一个borker就表示kafka集群中的一个节点==</p></blockquote> <p>topic</p> <blockquote><p>每条发布到Kafka集群的消息属于的类别，即Kafka是面向 topic 的。更通俗的说Topic就像一个消息队列，生产者可以向其写入消息，消费者可以从中读取消息，一个Topic支持多个生产者或消费者同时订阅它，所以其扩展性很好。</p></blockquote> <p>partition</p> <blockquote><p>==每个 topic 包含一个或多个partition==。Kafka分配的单位是partition</p></blockquote> <p>replica</p> <blockquote><p>partition的副本，保障 partition 的高可用。</p></blockquote> <p>consumer</p> <blockquote><p>从Kafka集群中消费消息的终端或服务</p></blockquote> <p>consumer group</p> <blockquote><p>每个 consumer 都属于一个 consumer group，==每条消息只能被 consumer group 中的一个 Consumer 消费，但可以被多个 consumer group 消费。==</p></blockquote> <p>leader</p> <blockquote><p>每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。 producer 和 consumer 只跟 leader 交互</p></blockquote> <p>follower</p> <blockquote><p>Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。</p></blockquote> <p>controller</p> <blockquote><p>大家有没有思考过一个问题，就是Kafka集群中某个broker宕机之后，是谁负责感知到他的宕机，以及负责进行Leader Partition的选举？如果你在Kafka集群里新加入了一些机器，此时谁来负责把集群里的数据进行负载均衡的迁移？包括你的Kafka集群的各种元数据，比如说每台机器上有哪些partition，谁是leader，谁是follower，是谁来管理的？如果你要删除一个topic，那么背后的各种partition如何删除，是谁来控制？还有就是比如Kafka集群扩容加入一个新的broker，是谁负责监听这个broker的加入？如果某个broker崩溃了，是谁负责监听这个broker崩溃？</p> <p>这里就需要一个==Kafka集群的总控组件，Controller。他负责管理整个Kafka集群范围内的各种东西。==</p></blockquote> <p>zookeeper</p> <blockquote><p>(1)Kafka 通过 zookeeper 来存储集群的meta元数据信息</p> <p>(2)一旦controller所在broker宕机了，此时临时节点消失，集群里其他broker会一直监听这个临时节点，发现临时节点消失了，就争抢再次创建临时节点，保证有一台新的broker会成为controller角色。</p></blockquote> <p>offset偏移量</p> <blockquote><p>消费者在对应分区上已经消费的消息数（位置），offset保存的地方跟kafka版本有一定的关系。</p> <p>kafka0.8 版本之前offset保存在zookeeper上。</p> <p>kafka0.8 版本之后offset保存在kafka集群上。它是把消费者消费topic的位置通过==kafka集群内部有一个默认的topic，名称叫__consumer_offsets==，它默认有50个分区。</p></blockquote> <p>ISR机制</p> <blockquote><p>光是依靠多副本机制能保证Kafka的高可用性，但是能保证数据不丢失吗？不行，因为如果leader宕机，但是leader的数据还没同步到follower上去，此时即使选举了follower作为新的leader，当时刚才的数据已经丢失了。</p> <p>ISR是：in-sync replica，就是跟leader partition保持同步的follower partition的数量，==只有处于ISR列表中的follower才可以在leader宕机之后被选举为新的leader==，因为在这个ISR列表里代表他的数据跟leader是同步的。</p></blockquote> <p>消费者consumer与partition的关系</p> <blockquote><p>假设有2个消费组，一个关于订单数据的topic有3个partition。两个消费组都要对该topic的数据进行处理。</p> <ol><li>如果一个消费组的消费者个数等于分区数，partition与这个消费组的消费者是一一对应关系。即一个消费者处理一个分区的数据</li> <li>如果一个消费组的消费者个数大于分区数，该组会出现消费者空闲状态，后期有些消费者挂掉之后，空闲状态的消费者会顶上去，继续消费数据。</li> <li>如果一个消费组的消费者个数小于分区数，这里会出现某个消费者消费多个分区数据的情况</li> <li>也就是说，topic中的每一条消息都只能被一个消费组中的某个消费者去消费，而不同的消费组是可以消费同一条消息的。</li></ol> <p>详情看下图：</p></blockquote> <p><img src="/assets/img/image-20200419214847181.050d87dd.png" alt="image-20200419214847181"></p> <h2 id="kafka集群安装部署"><a href="#kafka集群安装部署" class="header-anchor">#</a> kafka集群安装部署</h2> <p>需要事先安装好Zookeeper集群</p> <h5 id="下载安装包-http-kafka-apache-org"><a href="#下载安装包-http-kafka-apache-org" class="header-anchor">#</a> 下载安装包（http://kafka.apache.org）</h5> <p>spark是根据scala开发的，下载版本是2.11.xxx</p> <p>kafka也是根据scala开发的，下载版本也要对应上来，安装版本是2.11.xxx</p> <p>这里安装的kafka版本是2.11-1.0.1，是0.8版本之后的版本。</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token builtin class-name">cd</span> /kkb/soft
rz <span class="token comment">#上传安装包kafka_2.11-1.0.1.tgz到node01</span>
</code></pre></div><h5 id="解压安装包到指定规划目录"><a href="#解压安装包到指定规划目录" class="header-anchor">#</a> 解压安装包到指定规划目录</h5> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token function">tar</span> -zxvf /kkb/soft/kafka_2.11-1.0.1.tgz -C /kkb/install/
</code></pre></div><h5 id="重命名解压目录"><a href="#重命名解压目录" class="header-anchor">#</a> 重命名解压目录</h5> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token builtin class-name">cd</span> /kkb/install/
<span class="token function">mv</span> kafka_2.11-1.0.1 kafka
</code></pre></div><h5 id="修改配置文件"><a href="#修改配置文件" class="header-anchor">#</a> 修改配置文件</h5> <p>进入到kafka安装目录下有一个config目录</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token builtin class-name">cd</span> /kkb/install/kafka/config
<span class="token function">vi</span> server.properties
</code></pre></div><div class="language-shell extra-class"><pre class="language-shell"><code><span class="token comment">#指定kafka对应的broker id ，唯一</span>
broker.id<span class="token operator">=</span><span class="token number">0</span>
<span class="token comment">#指定数据存放的目录</span>
log.dirs<span class="token operator">=</span>/kkb/install/kafka/kafka-logs
<span class="token comment">#指定zk地址</span>
zookeeper.connect<span class="token operator">=</span>node01:2181,node02:2181,node03:2181
<span class="token comment">#指定是否可以删除topic ,默认是false 表示不可以删除</span>
delete.topic.enable<span class="token operator">=</span>true
<span class="token comment">#指定broker主机名</span>
host.name<span class="token operator">=</span>node01
</code></pre></div><p>注意：还要将该文件中的log.dirs=/tmp/kafka-logs一行注释掉，否则topic的数据的存放目录是/tmp/kafka-logs</p> <h5 id="ndoe01配置kafka环境变量"><a href="#ndoe01配置kafka环境变量" class="header-anchor">#</a> ndoe01配置kafka环境变量</h5> <p>sudo vi /etc/profile</p> <div class="language- extra-class"><pre class="language-text"><code>export KAFKA_HOME=/kkb/install/kafka
export PATH=$PATH:$KAFKA_HOME/bin
</code></pre></div><h5 id="分发kafka安装目录到其他节点"><a href="#分发kafka安装目录到其他节点" class="header-anchor">#</a> 分发kafka安装目录到其他节点</h5> <div class="language- extra-class"><pre class="language-text"><code>scp -r kafka node02:/kkb/install
scp -r kafka node03:/kkb/install
</code></pre></div><p>配置node03/node02的kafka环境变量</p> <div class="language- extra-class"><pre class="language-text"><code>sudo vi /etc/profile
</code></pre></div><h5 id="修改node02和node03上的配置"><a href="#修改node02和node03上的配置" class="header-anchor">#</a> 修改node02和node03上的配置</h5> <p>node02</p> <p>vi server.properties</p> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token comment">#指定kafka对应的broker id ，唯一</span>
broker.id<span class="token operator">=</span><span class="token number">1</span>
<span class="token comment">#指定数据存放的目录</span>
log.dirs<span class="token operator">=</span>/kkb/install/kafka/kafka-logs
<span class="token comment">#指定zk地址</span>
zookeeper.connect<span class="token operator">=</span>node01:2181,node02:2181,node03:2181
<span class="token comment">#指定是否可以删除topic ,默认是false 表示不可以删除</span>
delete.topic.enable<span class="token operator">=</span>true
<span class="token comment">#指定broker主机名</span>
host.name<span class="token operator">=</span>node02
</code></pre></div><p>node03</p> <p>vi server.properties</p> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token comment">#指定kafka对应的broker id ，唯一</span>
broker.id<span class="token operator">=</span><span class="token number">2</span>
<span class="token comment">#指定数据存放的目录</span>
log.dirs<span class="token operator">=</span>/kkb/install/kafka/kafka-logs
<span class="token comment">#指定zk地址</span>
zookeeper.connect<span class="token operator">=</span>node01:2181,node02:2181,node03:2181
<span class="token comment">#指定是否可以删除topic ,默认是false 表示不可以删除</span>
delete.topic.enable<span class="token operator">=</span>true
<span class="token comment">#指定broker主机名</span>
host.name<span class="token operator">=</span>node03
</code></pre></div><p>安装完后broker.id与主机的对应关系是：</p> <blockquote><p>node01----&gt;broker.id=0</p> <p>node02----&gt;broker.id=1</p> <p>node03----&gt;broker.id=2</p></blockquote> <h2 id="kafka集群启动和停止"><a href="#kafka集群启动和停止" class="header-anchor">#</a> kafka集群启动和停止</h2> <p>启动kafka时确保启动了zookeeper集群</p> <h5 id="启动"><a href="#启动" class="header-anchor">#</a> 启动</h5> <p>1、前台启动,三台机器执行下列代码：</p> <div class="language-sh extra-class"><pre class="language-sh"><code>kafka-server-start.sh /kkb/install/kafka/config/server.properties
</code></pre></div><p>2、后台启动，三台机器执行下列代码：</p> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token function">nohup</span> kafka-server-start.sh /kkb/install/kafka/config/server.properties <span class="token operator">&gt;</span>/dev/null <span class="token operator"><span class="token file-descriptor important">2</span>&gt;</span><span class="token file-descriptor important">&amp;1</span> <span class="token operator">&amp;</span>
</code></pre></div><p>3、写脚本来一键启动kafka</p> <div class="language- extra-class"><pre class="language-text"><code>vi /home/hadoop/bin/start_kafka.sh
</code></pre></div><div class="language-shell extra-class"><pre class="language-shell"><code><span class="token shebang important">#!/bin/sh</span>
<span class="token keyword">for</span> <span class="token for-or-select variable">host</span> <span class="token keyword">in</span> node01 node02 node03
<span class="token keyword">do</span>
        <span class="token function">ssh</span> <span class="token variable">$host</span> <span class="token string">&quot;source /etc/profile;nohup kafka-server-start.sh /kkb/install/kafka/config/server.properties &gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span> 
        <span class="token builtin class-name">echo</span> <span class="token string">&quot;<span class="token variable">$host</span> kafka is running&quot;</span>

<span class="token keyword">done</span>
</code></pre></div><h5 id="停止"><a href="#停止" class="header-anchor">#</a> 停止</h5> <p>所有节点执行关闭kafka脚本</p> <div class="language- extra-class"><pre class="language-text"><code>kafka-server-stop.sh
</code></pre></div><p>写脚本来一键停止kafka</p> <p>vi /home/hadoop/bin/stop_kafka.sh</p> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token shebang important">#!/bin/sh</span>
<span class="token keyword">for</span> <span class="token for-or-select variable">host</span> <span class="token keyword">in</span> node01 node02 node03
<span class="token keyword">do</span>
  <span class="token function">ssh</span> <span class="token variable">$host</span> <span class="token string">&quot;source /etc/profile;nohup /kkb/install/kafka/bin/kafka-server-stop.sh &amp;&quot;</span> 
  <span class="token builtin class-name">echo</span> <span class="token string">&quot;<span class="token variable">$host</span> kafka is stopping&quot;</span>
<span class="token keyword">done</span>
</code></pre></div><h5 id="一键启动和停止kafka脚本"><a href="#一键启动和停止kafka脚本" class="header-anchor">#</a> 一键启动和停止kafka脚本</h5> <div class="language- extra-class"><pre class="language-text"><code>vi /home/hadoop/bin/kafkaCluster.sh 
</code></pre></div><div class="language-shell extra-class"><pre class="language-shell"><code><span class="token shebang important">#!/bin/sh</span>
<span class="token keyword">case</span> <span class="token variable">$1</span> <span class="token keyword">in</span>   <span class="token comment">#$1代表第一个参数</span>
<span class="token string">&quot;start&quot;</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
<span class="token keyword">for</span> <span class="token for-or-select variable">host</span> <span class="token keyword">in</span> node01 node02 node03 
<span class="token keyword">do</span>
  <span class="token function">ssh</span> <span class="token variable">$host</span> <span class="token string">&quot;source /home/hadoop/.bash_profile;source /etc/profile; nohup /kkb/install/kafka/bin/kafka-server-start.sh /kkb/install/kafka/config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;&quot;</span>   
  <span class="token builtin class-name">echo</span> <span class="token string">&quot;<span class="token variable">$host</span> kafka is running...&quot;</span>  
<span class="token keyword">done</span>  
<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token punctuation">;</span>

<span class="token string">&quot;stop&quot;</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
<span class="token keyword">for</span> <span class="token for-or-select variable">host</span> <span class="token keyword">in</span> node01 node02 node03 
<span class="token keyword">do</span>
  <span class="token function">ssh</span> <span class="token variable">$host</span> <span class="token string">&quot;source /home/hadoop/.bash_profile;source /etc/profile; nohup /kkb/install/kafka/bin/kafka-server-stop.sh &gt;/dev/null  2&gt;&amp;1 &amp;&quot;</span>   
  <span class="token builtin class-name">echo</span> <span class="token string">&quot;<span class="token variable">$host</span> kafka is stopping...&quot;</span>  
<span class="token keyword">done</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token punctuation">;</span>
<span class="token keyword">esac</span>
</code></pre></div><p>写成这样也可以：</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token shebang important">#!/bin/sh</span>
<span class="token keyword">case</span> <span class="token variable">$1</span> <span class="token keyword">in</span>   <span class="token comment">#$1代表第一个参数</span>
<span class="token string">&quot;start&quot;</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
<span class="token keyword">for</span> <span class="token variable"><span class="token punctuation">((</span> i<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> <span class="token number">3</span> <span class="token punctuation">;</span> i <span class="token operator">=</span> $i <span class="token operator">+</span> <span class="token number">1</span> <span class="token punctuation">))</span></span><span class="token punctuation">;</span>
<span class="token keyword">do</span>
  <span class="token function">ssh</span> node0<span class="token variable">$i</span> <span class="token string">&quot;source /home/hadoop/.bash_profile;source /etc/profile; nohup /kkb/install/kafka/bin/kafka-server-start.sh /kkb/install/kafka/config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;&quot;</span>
  <span class="token builtin class-name">echo</span> <span class="token string">&quot;<span class="token variable">$host</span> kafka is running...&quot;</span>
<span class="token keyword">done</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token punctuation">;</span>

<span class="token string">&quot;stop&quot;</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
<span class="token keyword">for</span> <span class="token for-or-select variable">host</span> <span class="token keyword">in</span> node01 node02 node03
<span class="token keyword">do</span>
  <span class="token function">ssh</span> <span class="token variable">$host</span> <span class="token string">&quot;source /home/hadoop/.bash_profile;source /etc/profile; nohup /kkb/install/kafka/bin/kafka-server-stop.sh &gt;/dev/null  2&gt;&amp;1 &amp;&quot;</span>
  <span class="token builtin class-name">echo</span> <span class="token string">&quot;<span class="token variable">$host</span> kafka is stopping...&quot;</span>
<span class="token keyword">done</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token punctuation">;</span>
<span class="token keyword">esac</span>
</code></pre></div><p>一键启动kafka</p> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token function">sh</span> /home/bin/kafkaCluster.sh start
</code></pre></div><p>一键停止kafka</p> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token function">sh</span> /home/bin/kafkaCluster.sh stop
</code></pre></div><blockquote><p>说明：</p> <ol><li>之前写脚本的时候，因为只用了source /etc/profile; 导致node01和node02都不能使用脚本启动，后来加上了source /home/hadoop/.bash_profile;才把问题给解决了。可能是因为java环境变量设置在了/home/hadoop/.bash_profile的原因。</li></ol></blockquote> <h2 id="kafka的命令行的管理使用"><a href="#kafka的命令行的管理使用" class="header-anchor">#</a> kafka的命令行的管理使用</h2> <h5 id="_1、创建topic"><a href="#_1、创建topic" class="header-anchor">#</a> 1、创建topic</h5> <div class="language-shell extra-class"><pre class="language-shell"><code>kafka-topics.sh --create --partitions <span class="token number">3</span> --replication-factor <span class="token number">2</span> --topic <span class="token builtin class-name">test</span> --zookeeper node01:2181,node02:2181,node03:2181
</code></pre></div><h5 id="_2、查询所有的topic"><a href="#_2、查询所有的topic" class="header-anchor">#</a> 2、查询所有的topic</h5> <div class="language-shell extra-class"><pre class="language-shell"><code>kafka-topics.sh --list --zookeeper node01:2181,node02:2181,node03:2181 
<span class="token builtin class-name">test</span>
</code></pre></div><blockquote><p>说明：</p> <ol><li>kafka是有一个内置的topic的，名称为__consumer_offsets，但是现在还没创建。</li> <li>__consumer_offsets作为kafka的内部topic，用来保存消费组元数据以及对应提交的offset信息。</li> <li>消费者poll数据时会去创建consumer_offsets，当然前提是consumer_offsets不存在的情况下</li></ol></blockquote> <h5 id="_3、查看topic的描述信息"><a href="#_3、查看topic的描述信息" class="header-anchor">#</a> 3、查看topic的描述信息</h5> <div class="language-shell extra-class"><pre class="language-shell"><code>kafka-topics.sh --describe --topic <span class="token builtin class-name">test</span> --zookeeper node01:2181,node02:2181,node03:2181
</code></pre></div><blockquote><p>运行结果为：</p> <p>Topic:test      PartitionCount:3        ReplicationFactor:2     Configs:
Topic: test     Partition: 0    Leader: 1       Replicas: 1,2   Isr: 1,2
Topic: test     Partition: 1    Leader: 2       Replicas: 2,0   Isr: 2,0
Topic: test     Partition: 2    Leader: 0       Replicas: 0,1   Isr: 0,1</p> <p>说明：</p> <ol><li><p>PartitionCount:3 表示该topic一共3个partition</p></li> <li><p>ReplicationFactor:2 表示每个分区都有2个副本</p></li> <li><p>Partition: 0    Leader: 1       Replicas: 1,2   Isr: 1,2</p> <p>表示0号分区的Leader partition处在broker.id=1的节点上（node02),2个副本分别处在broker.id=1/broker.id=2的节点上（node02/node03), Isr列表有两个partition且分别处在broker.id=1/broker.id=2的节点上（node02/node03)</p></li> <li><p>Isr列表中的partition是跟leader partition的数据同步的，详情看kafka集群架构。</p></li></ol> <p>了解test topic的信息后，我们来==观察一下kafka的数据保存目录==：</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token punctuation">[</span>hadoop@node01 ~<span class="token punctuation">]</span>$ xcall <span class="token function">ls</span> /kkb/install/kafka/kafka-logs/
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span> node01 <span class="token function">ls</span> /kkb/install/kafka/kafka-logs/ <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
test-1
test-2
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span> node02 <span class="token function">ls</span> /kkb/install/kafka/kafka-logs/ <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
test-0
test-2
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span> node03 <span class="token function">ls</span> /kkb/install/kafka/kafka-logs/ <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
test-0
test-1
</code></pre></div><p>从ls结果可以发现，node01/node02/node03的/kkb/install/kafka/kafka-logs/数据保存目录下分别有不同的test-x目录，这个刚好跟描述信息符合。</p> <p>==topic的数据是以分区为单位进行保存的，而且一个分区的数据是保存在以该分区编号为后缀的目录下的==，比如test topic的0号partition的数据是保存在test-0目录下的。而0号分区有2个副本，分别在node02/node03,Leader partition在node2。</p></blockquote> <h5 id="_4、删除topic"><a href="#_4、删除topic" class="header-anchor">#</a> 4、删除topic</h5> <div class="language-shell extra-class"><pre class="language-shell"><code>kafka-topics.sh --delete --topic <span class="token builtin class-name">test</span> --zookeeper node01:2181,node02:2181,node03:2181 
</code></pre></div><h5 id="_5、模拟生产者写入数据到topic中"><a href="#_5、模拟生产者写入数据到topic中" class="header-anchor">#</a> 5、模拟生产者写入数据到topic中</h5> <p>kafka-console-producer.sh</p> <div class="language-shell extra-class"><pre class="language-shell"><code>kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic <span class="token builtin class-name">test</span> 
</code></pre></div><h5 id="_6、模拟消费者拉取topic中的数据"><a href="#_6、模拟消费者拉取topic中的数据" class="header-anchor">#</a> 6、模拟消费者拉取topic中的数据</h5> <p>kafka-console-consumer.sh</p> <div class="language-shell extra-class"><pre class="language-shell"><code>kafka-console-consumer.sh --zookeeper node01:2181,node02:2181,node03:2181 --topic <span class="token builtin class-name">test</span> --from-beginning
<span class="token comment">#或者</span>
kafka-console-consumer.sh --bootstrap-server node01:9092,node02:9092,node03:9092 --topic <span class="token builtin class-name">test</span> --from-beginning
</code></pre></div><blockquote><p>说明：</p> <p>在使用kafka-console-consumer.sh --zookeeper node01:2181,node02:2181,node03:2181 --topic test --from-beginning这种方式消费数据时，会报以下警告：</p> <p>Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. ==Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].==</p> <p>警告信息表示这种消费方式过时了，建议使用bootstrap-server。</p> <ol><li>当使用--zookeeper方式消费时，消费的偏移量等信息是保存在zookeeper中的</li> <li>当使用--bootstrap-server方式消费时，消费的偏移量等信息时保存在kafka的内置topic中的。</li></ol></blockquote> <h2 id="kafka的生产者api代码开发"><a href="#kafka的生产者api代码开发" class="header-anchor">#</a> kafka的生产者api代码开发</h2> <p>创建maven工程引入依赖</p> <div class="language-xml extra-class"><pre class="language-xml"><code>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.kafka<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>kafka-clients<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.0.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
</code></pre></div><p>代码开发</p> <div class="language-java extra-class"><pre class="language-java"><code><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token class-name">KafkaProducer</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token class-name">Producer</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token class-name">ProducerRecord</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">KafkaDemo</span> <span class="token punctuation">{</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token comment">//添加属性，设置kafka集群地址</span>
        <span class="token class-name">Properties</span> props<span class="token operator">=</span><span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">&quot;bootstrap.servers&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;node01:9092,node02:9092,node03:9092&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;ack&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;1&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;retries&quot;</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;buffer.memory&quot;</span><span class="token punctuation">,</span><span class="token number">33554432</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;batch.size&quot;</span><span class="token punctuation">,</span><span class="token number">16384</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;linger.ms&quot;</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//消息是由key和value构成的，下面设置key和value的序列化器</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;key.serializer&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;value.serializer&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token class-name">Producer</span> producer<span class="token operator">=</span><span class="token keyword">new</span> <span class="token class-name">KafkaProducer</span><span class="token punctuation">(</span>props<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//或Producer&lt;String,String&gt; producer=new KafkaProducer&lt;String,String&gt;(props);</span>

        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token number">100</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
            <span class="token comment">//下面需要三个参数，第一个：topic的名称，第二个参数：表示消息的key,第三个参数：消息具体内容</span>
            producer<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token punctuation">(</span><span class="token string">&quot;test&quot;</span><span class="token punctuation">,</span><span class="token class-name">Integer</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">&quot;hello-kafka-&quot;</span><span class="token operator">+</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        producer<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

</code></pre></div><blockquote><p>说明：</p> <ol><li><p>props.put(&quot;==ack==&quot;,&quot;1&quot;); ack代表==消息确认机制==，有3种值：1 0 -1
ack = 0: 表示produce发送完消息后，请求立即返回，不需要等待leader的任何确认。这种方案有最高的吞吐率，但是不保证消息是否真的发送成功。</p> <p>ack = 1: 表示leader副本必须应答此produce请求并写入消息到本地日志，之后produce请求被认为成功. 如果leader挂掉有数据丢失的风险</p> <p>ack = -1或者all: 表示分区leader必须等待消息被成功写入到所有的ISR副本(同步副本)中才认为produce请求成功。这种方案提供最高的消息持久性保证，但是理论上吞吐率也是最差的。</p></li> <li><p>props.put(&quot;retries&quot;,0);表示==生产者发送消息失败后，重试的次数==。</p></li> <li><p>props.put(&quot;buffer.memory&quot;,33554432); buffer.memory是缓冲区大小，33554432表示32M。 props.put(&quot;batch.size&quot;,16384); batch.size是批处理大小，16384表示16KB。</p> <p>详情见下图：producer如果一条条地把数据发送到topic，会比较费力，所以producer先往缓冲区写数据，缓冲区满32M时就开始进行批次发送，每一批次发送16KB数据。</p> <p><img src="/assets/img/image-20200420042106148.e31a1fba.png" alt="image-20200420042106148"></p></li> <li><p>props.put(&quot;linger.ms&quot;, 1); linger.ms表示如果批次发送的batch不满16KB时，不会立即发送数据出去，而是等待1ms,1ms后，如果凑够16KB就发送16KB数据过去，当然不凑够也会发送出去。</p></li> <li><p>producer.send(new ProducerRecord(xxx))将数据发送出去，new ProducerRecord(xxx)将一条数据包装成ProducerRecord</p></li></ol> <p>==模拟消费者消费数据验证producer是否成功发送数据：==</p> <div class="language-sh extra-class"><pre class="language-sh"><code>kafka-console-consumer.sh --bootstrap-server node01:9092,node02:9092,node03:9092 --topic <span class="token builtin class-name">test</span> --from-beginning
</code></pre></div><p><img src="/assets/img/image-20200420043548757.91606e49.png" alt="image-20200420043548757"></p></blockquote> <h2 id="kafka的消费者api代码开发"><a href="#kafka的消费者api代码开发" class="header-anchor">#</a> kafka的消费者api代码开发</h2> <h4 id="自动提交偏移量代码开发"><a href="#自动提交偏移量代码开发" class="header-anchor">#</a> 自动提交偏移量代码开发</h4> <div class="language-java extra-class"><pre class="language-java"><code><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerRecord</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerRecords</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">KafkaConsumer</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Arrays</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">KafkaDemo2</span> <span class="token punctuation">{</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token class-name">Properties</span> props<span class="token operator">=</span><span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">&quot;bootstrap.servers&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;node01:9092,node02:9092,node03:9092&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//指定消费者组id</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;group.id&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;consumer-test&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//指定是否自动提交偏移量</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;enable.auto.commit&quot;</span><span class="token punctuation">,</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//自动提交偏移量的时间间隔：</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;enable.auto.interval.ms&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;1000&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;auto.offset.reset&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;earliest&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//指定key和value的反序列化器</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;key.deserializer&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;value.deserializer&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>props<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//指定消费哪些topic</span>
        consumer<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//设置死循环，持续开启消费</span>
        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> records <span class="token operator">=</span> consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token comment">//消费数据：</span>
            <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token class-name">ConsumerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span><span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> r<span class="token operator">:</span>records<span class="token punctuation">)</span><span class="token punctuation">{</span>
                <span class="token keyword">int</span> partition<span class="token operator">=</span>r<span class="token punctuation">.</span><span class="token function">partition</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token class-name">String</span> key<span class="token operator">=</span>r<span class="token punctuation">.</span><span class="token function">key</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token class-name">String</span> value<span class="token operator">=</span>r<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token keyword">long</span> offset<span class="token operator">=</span>r<span class="token punctuation">.</span><span class="token function">offset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;partition:&quot;</span><span class="token operator">+</span>partition<span class="token operator">+</span><span class="token string">&quot;\t key:&quot;</span><span class="token operator">+</span>key<span class="token operator">+</span><span class="token string">&quot;\toffset:&quot;</span><span class="token operator">+</span>offset<span class="token operator">+</span><span class="token string">&quot;\tvalue:&quot;</span><span class="token operator">+</span>value<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

</code></pre></div><blockquote><p>说明：</p> <ol><li><p>props.put(&quot;auto.offset.reset&quot;,&quot;earliest&quot;); auto.offset.reset表示偏移量自动重置的方式，有三种方式,默认是latest方式，如下：</p> <p>earliest: 当各分区下有已提交的offset时，从提交的offset开始消费；==无提交的offset时，从头开始消费==
latest: 当各分区下有已提交的offset时，从提交的offset开始消费；==无提交的offset时，消费新产生的该分区下的数据==
none : topic各分区都存在已提交的offset时，从offset后开始消费；==只要有一个分区不存在已提交的offset，则抛出异常==</p></li> <li><p><strong>使用consumer.poll(100)拉取数据时，会将拉取到的每一条消息数据封装在ConsumerRecord对象里，而所有的ConsumerRecord对象都被封装在一个ConsumerRecords类对象里。</strong></p></li> <li><p>consumer.poll(100)的参数100表示timeout,是一个超时时间，消费者一旦拿到足够多的数据（具体多少看参数设置），consumer.poll(100)就会返回ConsumerRecords&lt;String,String&gt;，==如果没有拿到足够多的数据，会阻塞100ms,但是100ms后就会返回==。</p></li></ol> <p>运行后，消费了数据，而且程序一直保持运行状态：</p> <p><img src="data:image/png;base64,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" alt="image-20200420144834785"></p></blockquote> <h4 id="手动提交偏移量代码开发"><a href="#手动提交偏移量代码开发" class="header-anchor">#</a> 手动提交偏移量代码开发</h4> <div class="language-java extra-class"><pre class="language-java"><code><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerRecord</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">ConsumerRecords</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token class-name">KafkaConsumer</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">ArrayList</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Arrays</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">KafkaDemo2</span> <span class="token punctuation">{</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token class-name">Properties</span> props<span class="token operator">=</span><span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">&quot;bootstrap.servers&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;node01:9092,node02:9092,node03:9092&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;group.id&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;manual-consumer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;enable.auto.commit&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;false&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;key.deserializer&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;value.deserializer&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>props<span class="token punctuation">)</span><span class="token punctuation">;</span>
        consumer<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//定义一个数字，表示消费多少条消息后手动提交偏移量</span>
        <span class="token keyword">final</span> <span class="token keyword">int</span> minBatchSize<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">;</span>

        <span class="token comment">//定义一个数组，缓冲一批数据</span>
        <span class="token class-name">ArrayList</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">ConsumerRecord</span><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span><span class="token punctuation">&gt;</span></span> buffer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">ConsumerRecord</span><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> records <span class="token operator">=</span> consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token comment">//将拉取的每一条数据都添加到buffer</span>
            <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token class-name">ConsumerRecord</span> cr<span class="token operator">:</span>records<span class="token punctuation">)</span><span class="token punctuation">{</span>
                buffer<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>cr<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
            <span class="token comment">//消费数据：</span>
            <span class="token keyword">if</span><span class="token punctuation">(</span>buffer<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;=</span>minBatchSize<span class="token punctuation">)</span><span class="token punctuation">{</span>
                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;缓冲区的数据条数：&quot;</span><span class="token operator">+</span>buffer<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;我已经处理完这一批数据了...&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                consumer<span class="token punctuation">.</span><span class="token function">commitSync</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//提交偏移量</span>
                buffer<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

</code></pre></div><p>运行本程序后，使用生产者程序发送消息，然后观看本程序运行界面，出现如下输出：</p> <div class="language-sh extra-class"><pre class="language-sh"><code>缓冲区的数据条数：20
我已经处理完这一批数据了<span class="token punctuation">..</span>.
缓冲区的数据条数：27
我已经处理完这一批数据了<span class="token punctuation">..</span>.
缓冲区的数据条数：45
我已经处理完这一批数据了<span class="token punctuation">..</span>.
缓冲区的数据条数：33
我已经处理完这一批数据了<span class="token punctuation">..</span>.
</code></pre></div><h2 id="招聘要求介绍"><a href="#招聘要求介绍" class="header-anchor">#</a> 招聘要求介绍</h2> <p><img src="/assets/img/1568432422326.9ece77a0.png" alt="1568432422326"></p> <p><img src="/assets/img/1568432464259.892f14be.png" alt="1568432464259"></p> <h2 id="kafka分区策略"><a href="#kafka分区策略" class="header-anchor">#</a> kafka分区策略</h2> <h5 id="哪里指定分区策略"><a href="#哪里指定分区策略" class="header-anchor">#</a> 哪里指定分区策略？</h5> <p>kafka的分区策略决定了producer生产者产生的一条消息最后会写入到topic的哪一个分区中。</p> <p>分区策略在ProducerRecord类的对象里设置，我们来看一下源码，如下。我们可以看到，ProducerRecord类有多种构造方法。使用不同的构造方法构建对象时，会有不同的分区策略。</p> <div class="language-java extra-class"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token class-name">ProducerRecord</span><span class="token punctuation">(</span><span class="token class-name">String</span> topic<span class="token punctuation">,</span> <span class="token class-name">Integer</span> partition<span class="token punctuation">,</span> <span class="token class-name">Long</span> timestamp<span class="token punctuation">,</span> <span class="token class-name">K</span> key<span class="token punctuation">,</span> <span class="token class-name">V</span> value<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">this</span><span class="token punctuation">(</span>topic<span class="token punctuation">,</span> partition<span class="token punctuation">,</span> timestamp<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Iterable</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token class-name">ProducerRecord</span><span class="token punctuation">(</span><span class="token class-name">String</span> topic<span class="token punctuation">,</span> <span class="token class-name">Integer</span> partition<span class="token punctuation">,</span> <span class="token class-name">K</span> key<span class="token punctuation">,</span> <span class="token class-name">V</span> value<span class="token punctuation">,</span> <span class="token class-name">Iterable</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Header</span><span class="token punctuation">&gt;</span></span> headers<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">this</span><span class="token punctuation">(</span>topic<span class="token punctuation">,</span> partition<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Long</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> headers<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token class-name">ProducerRecord</span><span class="token punctuation">(</span><span class="token class-name">String</span> topic<span class="token punctuation">,</span> <span class="token class-name">Integer</span> partition<span class="token punctuation">,</span> <span class="token class-name">K</span> key<span class="token punctuation">,</span> <span class="token class-name">V</span> value<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">this</span><span class="token punctuation">(</span>topic<span class="token punctuation">,</span> partition<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Long</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Iterable</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token class-name">ProducerRecord</span><span class="token punctuation">(</span><span class="token class-name">String</span> topic<span class="token punctuation">,</span> <span class="token class-name">K</span> key<span class="token punctuation">,</span> <span class="token class-name">V</span> value<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">this</span><span class="token punctuation">(</span>topic<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Integer</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Long</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Iterable</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token class-name">ProducerRecord</span><span class="token punctuation">(</span><span class="token class-name">String</span> topic<span class="token punctuation">,</span> <span class="token class-name">V</span> value<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">this</span><span class="token punctuation">(</span>topic<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Integer</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Long</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Object</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">,</span> value<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">Iterable</span><span class="token punctuation">)</span><span class="token keyword">null</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
</code></pre></div><h5 id="_1、指定具体的分区号"><a href="#_1、指定具体的分区号" class="header-anchor">#</a> 1、指定具体的分区号</h5> <p>给定具体的分区号，数据就会写入到指定的分区中</p> <div class="language-java extra-class"><pre class="language-java"><code><span class="token comment">//将数据写入0号分区中：</span>
producer<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token string">&quot;test&quot;</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token class-name">Integer</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&quot;hello-kafka-&quot;</span><span class="token operator">+</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><h5 id="_2、不给定具体的分区号-给定key的值-key不断变化"><a href="#_2、不给定具体的分区号-给定key的值-key不断变化" class="header-anchor">#</a> 2、不给定具体的分区号，给定key的值（key不断变化）</h5> <p>不给定具体的分区号，给定一个key值, 这里使用计算公式：key.hashcode%分区数=分区号 来决定。key是消息的key。</p> <div class="language-java extra-class"><pre class="language-java"><code><span class="token comment">//Integer.toString(i)在这里是key</span>
producer<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token string">&quot;test&quot;</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&quot;hello-kafka-&quot;</span><span class="token operator">+</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><h5 id="_3、不给定具体的分区号-也不给对应的key"><a href="#_3、不给定具体的分区号-也不给对应的key" class="header-anchor">#</a> 3、不给定具体的分区号，也不给对应的key</h5> <p>不给定具体的分区号，也不给定对应的key ,这个它会进行轮训的方式把数据写入到不同分区中</p> <div class="language-java extra-class"><pre class="language-java"><code>producer<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token string">&quot;test&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;hello-kafka-&quot;</span><span class="token operator">+</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><h5 id="_4、自定义分区"><a href="#_4、自定义分区" class="header-anchor">#</a> 4、自定义分区</h5> <p>定义一个类实现接口Partitioner，并实现里面的抽象方法。</p> <div class="language-java extra-class"><pre class="language-java"><code><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token class-name">Partitioner</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>common<span class="token punctuation">.</span></span><span class="token class-name">Cluster</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Map</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MyPartitioner</span> <span class="token keyword">implements</span> <span class="token class-name">Partitioner</span><span class="token punctuation">{</span>
    <span class="token comment">/**
     *
     * @param s  topic
     * @param o  消息数据的key
     * @param bytes  key的字节数组
     * @param o1   消息数据的value
     * @param bytes1   value的字节数组
     * @param cluster  集群
     * @return
     */</span>
    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">partition</span><span class="token punctuation">(</span><span class="token class-name">String</span> s<span class="token punctuation">,</span> <span class="token class-name">Object</span> o<span class="token punctuation">,</span> <span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> bytes<span class="token punctuation">,</span> <span class="token class-name">Object</span> o1<span class="token punctuation">,</span> <span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> bytes1<span class="token punctuation">,</span> <span class="token class-name">Cluster</span> cluster<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token comment">//获取topic的分区个数：</span>
        <span class="token class-name">Integer</span> numPartitions <span class="token operator">=</span> cluster<span class="token punctuation">.</span><span class="token function">partitionCountForTopic</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//计算并返回分区号</span>
        <span class="token comment">// o.hashCode()%numPartitions的值可能是 -1 -2 0 1 2，要取绝对值</span>
        <span class="token keyword">return</span> <span class="token class-name">Math</span><span class="token punctuation">.</span><span class="token function">abs</span><span class="token punctuation">(</span>o<span class="token punctuation">.</span><span class="token function">hashCode</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">%</span>numPartitions<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>

    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span><span class="token class-name">Map</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token operator">?</span><span class="token punctuation">&gt;</span></span> map<span class="token punctuation">)</span> <span class="token punctuation">{</span>

    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre></div><p>配置自定义分区类</p> <div class="language-java extra-class"><pre class="language-java"><code><span class="token comment">//在Properties对象中添加自定义分区类</span>
props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;partitioner.class&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;com.kaikeba.partitioner.MyPartitioner&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><h2 id="kafka的文件存储机制"><a href="#kafka的文件存储机制" class="header-anchor">#</a> kafka的文件存储机制</h2> <p><img src="/assets/img/image-20200602221111202.1ded8814.png" alt="image-20200602221111202"></p> <h4 id="概述"><a href="#概述" class="header-anchor">#</a> 概述</h4> <blockquote><p>同一个topic下有多个不同的partition，每个partition为一个目录，partition命名的规则是topic的名称加上一个序号，序号从0开始。</p> <p><img src="/assets/img/image-20200421031834688.0b04a890.png" alt="image-20200421031834688"></p> <p>每一个partition目录下的文件被平均切割成大小相等（默认一个文件是1G，可以手动去设置）的数据文件，==每一个数据文件都被称为一个段（segment file）==，但每个段消息数量不一定相等。一个segment内部包含了2个文件，一个是.log，一个是.index，log负责数据的存储，index负责对文件数据构建索引，方便以后查询。这种特性能够使得老的segment可以被快速清除。==默认保留7天的数据。==</p> <p>segment每次满1G后，数据会再写入到一个新的文件中。</p> <img src="kafka.assets/image-20200421031850314.png" alt="image-20200421031850314" style="zoom:80%;"> <p>另外每个partition只需要支持顺序读写就可以。如上图所示：</p> <p>首先00000000000000000000.log是最早产生的文件，</p> <p>该文件达到1G后又产生了新的00000000000002025849.log文件，新的数据会写入到这个新的文件里面。这个文件到达1G后，数据又会写入到下一个文件中。也就是说它只会往文件的末尾追加数据，这就是顺序写的过程，生产者只会对每一个partition做数据的追加（写操作）。</p> <p>==新文件的文件的命名规则就是之前分区的所有数据的条数+1==</p></blockquote> <h4 id="数据消费问题讨论-面试题"><a href="#数据消费问题讨论-面试题" class="header-anchor">#</a> 数据消费问题讨论（面试题）</h4> <blockquote><p>问题：==如何保证消息消费的有序性呢？==比如说生产者生产了0到100个商品，那么消费者在消费的时候按照0到100这个从小到大的顺序消费？</p> <p>1、那么kafka如何保证这种有序性呢？</p> <p>难度就在于，生产者生产出0到100这100条数据之后，通过一定的分组策略存储到broker的partition中的时候，比如0到10这10条消息被存到了这个partition中，10到20这10条消息被存到了那个partition中，这样的话，消息在分组存到partition中的时候就已经被分组策略搞得无序了。</p> <p>2、那么能否做到消费者在消费消息的时候全局有序呢？</p> <p>遇到这个问题，我们可以回答，在大多数情况下是做不到全局有序的。但在某些情况下是可以做到的。比如我的partition只有一个，这种情况下是可以全局有序的。</p> <p>那么可能有人又要问了，只有一个partition的话，哪里来的分布式呢？哪里来的负载均衡呢？所以说，全局有序是一个伪命题！全局有序根本没有办法在kafka要实现的大数据的场景来做到。但是我们只能保证当前这个partition内部消息消费的有序性。</p> <p>==结论：一个partition中的数据是有序的吗？回答：间隔有序，不连续。==</p> <p>针对一个topic里面的数据，只能做到partition内部有序，不能做到全局有序。特别是加入消费者的场景后，如何保证消费者的消费的消息的全局有序性，这是一个伪命题，只有在一种情况下才能保证消费的消息的全局有序性，那就是只有一个partition。</p></blockquote> <h4 id="segment文件"><a href="#segment文件" class="header-anchor">#</a> Segment文件</h4> <h6 id="segment-file是什么"><a href="#segment-file是什么" class="header-anchor">#</a> Segment file是什么</h6> <blockquote><p>生产者生产的消息按照一定的分区策略被发送到topic中partition中，partition在磁盘上就是一个目录，该目录名是topic的名称加上一个序号，在这个==partition目录下，有两类文件，一类是以log为后缀的文件，一类是以index为后缀的文件，每一个log文件和一个index文件相对应==，这一对文件就是一个segment file，也就是一个段。</p> <p>其中的log文件就是数据文件，里面存放的就是消息，而index文件是索引文件，索引文件记录了元数据信息。log文件达到1个G后滚动重新生成新的log文件</p></blockquote> <h6 id="segment文件特点"><a href="#segment文件特点" class="header-anchor">#</a> Segment文件特点</h6> <blockquote><p>segment文件命名的规则：partition全局的第一个segment从0（20个0）开始，后续的每一个segment文件名是上一个segment文件中最后一条消息的offset值。那么这样命令有什么好处呢？</p> <p>==假如我们有一个消费者已经消费到了368776（offset值为368776），那么现在我们要继续消费的话，怎么做呢？==</p> <p>看下图，分2个步骤;</p> <p>第1步：从所有log文件的的文件名中找到对应的log文件，第368776条数据位于上图中的“00000000000000368769.log”这个文件中，这一步涉及到一个常用的算法叫做“二分查找法”</p> <p>（假如我现在给你一个offset值让你去找，你首先是==将所有的log的文件名进行排序，然后通过二分查找法进行查找，很快就能定位到某一个文件==，紧接着拿着这个offset值到其索引文件中找这条数据究竟存在哪里）；</p> <p>第2步：到index文件中去找第368776条数据所在的位置。</p> <p>索引文件（index文件）中存储这大量的元数据，而数据文件（log文件）中存储这大量的消息。</p> <p>索引文件（index文件）中的元数据指向对应的数据文件（log文件）中消息的物理偏移地址。</p> <p>那==如何根据索引文件来快速找到第368776条数据所在的位置？看下面的快速查询知识点。==</p></blockquote> <h2 id="kafka如何快速查询数据"><a href="#kafka如何快速查询数据" class="header-anchor">#</a> kafka如何快速查询数据</h2> <h4 id="index与log文件结构"><a href="#index与log文件结构" class="header-anchor">#</a> index与log文件结构</h4> <p><img src="/assets/img/kafka.c69b476a.png" alt="kafka"></p> <blockquote><p>上图的左半部分是索引文件，里面存储的是一对一对的key-value，其中key是消息在数据文件（对应的log文件）中的编号，比如==“1,3,6,8……”，分别表示在log文件中的第1条消息、第3条消息、第6条消息==、第8条消息……，那么为什么在index文件中这些编号不是连续的呢？</p> <p>这是因为==index文件中并没有为数据文件中的每条消息都建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引==。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。</p> <p>但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。</p> <p>例子1：以要读取第368777条数据为例， 由368777-368769=8得到，该数据在0000368769.log文件中是第8条数据，再找到索引文件中元数据8,1686，其中8代表在右边log数据文件中从上到下第8个消息(在全局partiton表示第368777个消息)，其中1686表示该消息的物理偏移地址（位置）为1686。</p> <p>要是读取offset=368777的消息，就从00000000000000368769.log文件中的1686的位置进行读取。</p> <p>例子2：如果要读取的是第368776条数据呢？368776-368769=7，表示log文件中的第7条数据，但在稀疏的index文件中没有为第7条数据建立索引，这时候就可以先定位到第6条或第8条数据在log文件的物理位置等，然后进行依次顺序扫描即可。虽说顺序扫描效率不够，但是扫描范围已经很小了。</p> <p>==那定位到某条数据在log文件的位置后，开始读取，那怎么知道何时读完本条消息，否则就读到下一条消息的内容了？==，看下面的Message物理结构。</p></blockquote> <h4 id="message物理结构"><a href="#message物理结构" class="header-anchor">#</a> Message物理结构</h4> <p><img src="/assets/img/20170107212325100.a034ab2d.png" alt="img"></p> <p>参数说明：</p> <table><thead><tr><th>关键字</th> <th>解释说明</th></tr></thead> <tbody><tr><td>8 byte offset</td> <td>在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message</td></tr> <tr><td>4 byte message size</td> <td>message大小</td></tr> <tr><td>4 byte CRC32</td> <td>用crc32校验message</td></tr> <tr><td>1 byte “magic&quot;</td> <td>表示本次发布Kafka服务程序协议版本号</td></tr> <tr><td>1 byte “attributes&quot;</td> <td>表示为独立版本、或标识压缩类型、或编码类型。</td></tr> <tr><td>4 byte key length</td> <td>表示key的长度,当key为-1时，K byte key字段不填</td></tr> <tr><td>K byte key</td> <td>可选</td></tr> <tr><td>value bytes payload</td> <td>表示实际消息数据。</td></tr></tbody></table> <blockquote><p>这个就需要涉及到消息的物理结构了，==消息都具有固定的物理结构==，包括：offset（8 Bytes）、消息体的大小（4 Bytes）、crc32（4 Bytes）、magic（1 Byte）、attributes（1 Byte）、key length（4 Bytes）、key（K Bytes）、payload(N Bytes)等等字段，==可以确定一条消息的大小，即读取到哪里截止。==</p></blockquote> <h4 id="kafka高效文件存储设计特点"><a href="#kafka高效文件存储设计特点" class="header-anchor">#</a> kafka高效文件存储设计特点</h4> <ol><li>Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。</li> <li>通过索引信息可以快速定位message</li> <li>通过将index元数据全部映射到memory，可以避免segment file的IO磁盘操作。</li> <li>通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</li></ol> <h2 id="为什么kafka速度那么快"><a href="#为什么kafka速度那么快" class="header-anchor">#</a> 为什么Kafka速度那么快</h2> <p>Kafka是大数据领域无处不在的消息中间件，目前广泛使用在企业内部的实时数据管道，并帮助企业构建自己的流计算应用程序。</p> <p>Kafka虽然是基于磁盘做的数据存储，但却具有高性能、高吞吐、低延时的特点，其吞吐量动辄几万、几十上百万，这其中的原由值得我们一探究竟。</p> <h4 id="顺序读写"><a href="#顺序读写" class="header-anchor">#</a> 顺序读写</h4> <blockquote><p>==磁盘顺序读写性能要高于内存的随机读写==</p> <p>众所周知Kafka是将消息记录持久化到本地磁盘中的，一般人会认为磁盘读写性能差，可能会对Kafka性能如何保证提出质疑。实际上不管是内存还是磁盘，快或慢关键在于寻址的方式，磁盘分为顺序读写与随机读写，内存也一样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读写性能却很高，一般而言要高出磁盘随机读写三个数量级，==一些情况下磁盘顺序读写性能甚至要高于内存随机读写==。</p> <p>磁盘的顺序读写是磁盘使用模式中最有规律的，并且操作系统也对这种模式做了大量优化，Kafka就是使用了磁盘顺序读写来提升的性能。==Kafka的message是不断追加到本地磁盘文件末尾的，而不是随机的写入，这使得Kafka写入吞吐量得到了显著提升==</p></blockquote> <h4 id="page-cache-页缓存"><a href="#page-cache-页缓存" class="header-anchor">#</a> Page Cache(页缓存)</h4> <blockquote><p>为了优化读写性能，==Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存==。这样做的好处有：</p> <p>（1）避免Object消耗：如果是使用Java堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。</p> <p>（2）避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题。</p></blockquote> <h4 id="零拷贝-sendfile-了解即可"><a href="#零拷贝-sendfile-了解即可" class="header-anchor">#</a> 零拷贝(sendfile)-了解即可</h4> <p>零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在IO读写过程中。Kafka利用linux操作系统的 &quot;零拷贝（zero-copy）&quot; 机制在消费端做的优化。</p> <h6 id="数据从文件发送到socket网络连接中的常规传输路径"><a href="#数据从文件发送到socket网络连接中的常规传输路径" class="header-anchor">#</a> 数据从文件发送到socket网络连接中的常规传输路径：</h6> <blockquote><p>比如：读取文件，再用socket发送出去。传统方式的实现是：</p> <ol><li>先读取、再发送，实际经过1~4四次copy。</li> <li>buffer = File.read</li> <li>Socket.send(buffer)</li></ol> <p>示意图如下：</p> <img src="kafka.assets/image-20200421134041498.png" alt="image-20200421134041498" style="zoom:67%;"> <p>第一步：操作系统从磁盘读取数据到内核空间（kernel space）的Page Cache缓冲区</p> <p>第二步：应用程序读取内核缓冲区的数据copy到用户空间（user space）的缓冲区</p> <p>第三步：应用程序将用户空间缓冲区的数据copy回内核空间到socket缓冲区</p> <p>第四步：操作系统将数据从socket缓冲区copy到网卡，由网卡进行网络传输</p> <p>拓展：套接字（<em>socket</em>）是一个抽象层，应用程序可以通过它发送或接收数据，可对其进行像对文件一样的打开、读写和关闭等操作。套接字允许应用程序将I/O插入到网络中，并与网络中的其他应用程序进行通信。</p> <p>==传统方式，读取磁盘文件并进行网络发送，经过的四次数据copy是非常繁琐的==。实际IO读写，需要进行IO中断，需要CPU响应中断(带来上下文切换)，尽管后来引入DMA来接管CPU的中断请求，但四次copy是存在“不必要的拷贝”的。</p> <p>重新思考传统IO方式，会注意到实际上并不需要第二个和第三个数据副本。应用程序除了缓存数据并将其传输回套接字缓冲区之外什么都不做。相反，数据可以直接从读缓冲区传输到套接字缓冲区。</p> <p>显然，第二次和第三次数据copy 其实在==这种场景==下没有什么帮助反而带来开销，这也正是零拷贝出现的意义。</p> <p>==这种场景：是指读取磁盘文件后，不需要做其他处理，直接用网络发送出去。试想，如果读取磁盘的数据需要用程序进一步处理的话，必须要经过第二次和第三次数据copy，让应用程序在内存缓冲区处理。==</p> <p>下面，我们来尝试做下改变，减少copy次数：</p> <img src="kafka.assets/image-20200421134751711.png" alt="image-20200421134751711" style="zoom:80%;"> <p>此时我们会发现用户态“空空如也”。数据没有来到用户态，而是直接在核心态就进行了传输，但这样依然还是有多次复制。首先数据被读取到read buffer中，然后发到socket buffer，最后才发到网卡。虽然减少了用户态和核心态的切换，但依然存在多次数据复制。</p> <p>如果可以进一步减少数据复制的次数，甚至没有数据复制是不是就会做到最快呢？</p></blockquote> <h6 id="利用dma实现-零拷贝"><a href="#利用dma实现-零拷贝" class="header-anchor">#</a> 利用DMA实现“零拷贝”</h6> <p>DMA，全称叫Direct Memory Access，一种可让某些硬件子系统去直接访问系统主内存，而不用依赖CPU的计算机系统的功能。听着是不是很厉害，跳过CPU，直接访问主内存。传统的内存访问都需要通过CPU的调度来完成。如下图：</p> <p><img src="/assets/img/image-20200421134217426.3b24e1e8.png" alt="image-20200421134217426"></p> <p>而DMA，则可以绕过CPU，硬件自己去直接访问系统主内存。如下图</p> <p><img src="/assets/img/1577687862577.968d214b.png" alt="1577687862577"></p> <p>回到本文中的文件传输，有了DMA后，就可以实现绝对的零拷贝了，因为网卡是直接去访问系统主内存的。如下图：</p> <p><img src="/assets/img/image-20200421135005858.fd334f7a.png" alt="image-20200421135005858"></p> <h4 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h4> <p>Kafka采用==顺序读写、Page Cache、零拷贝以及分区分段等这些设计，再加上在索引方面做的优化，另外Kafka数据读写也是批量的而不是单条的==，使得Kafka具有了高性能、高吞吐、低延时的特点。这样Kafka提供大容量的磁盘存储也变成了一种优点</p> <p>Java的NIO提供了FileChannle，它的transferTo、transferFrom方法就是Zero Copy。</p> <h2 id="kafka整合flume"><a href="#kafka整合flume" class="header-anchor">#</a> kafka整合flume</h2> <h4 id="为什么整合flume"><a href="#为什么整合flume" class="header-anchor">#</a> 为什么整合flume？</h4> <p>观察下面的开发思路就可以理解了：</p> <p><img src="/assets/img/image-20200420153016660.8a8cf37e.png" alt="image-20200420153016660"></p> <h4 id="kafka整合flume案例"><a href="#kafka整合flume案例" class="header-anchor">#</a> kafka整合flume案例</h4> <p>1、添加flume的配置</p> <p>vi flume-kafka.conf</p> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token comment">#为我们的source channel  sink起名</span>
a1.sources <span class="token operator">=</span> r1
a1.channels <span class="token operator">=</span> c1
a1.sinks <span class="token operator">=</span> k1

<span class="token comment">#指定我们的source数据收集策略</span>
a1.sources.r1.type <span class="token operator">=</span> spooldir
a1.sources.r1.spoolDir <span class="token operator">=</span> /kkb/install/flumeData/files
a1.sources.r1.inputCharset <span class="token operator">=</span> utf-8

<span class="token comment">#指定我们的source收集到的数据发送到哪个管道</span>
a1.sources.r1.channels <span class="token operator">=</span> c1

<span class="token comment">#指定我们的channel为memory,即表示所有的数据都装进memory当中</span>
a1.channels.c1.type <span class="token operator">=</span> memory
a1.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a1.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>


<span class="token comment">#指定我们的sink为kafka sink，并指定我们的sink从哪个channel当中读取数据</span>
a1.sinks.k1.channel <span class="token operator">=</span> c1
a1.sinks.k1.type <span class="token operator">=</span> org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic <span class="token operator">=</span> kaikeba
a1.sinks.k1.kafka.bootstrap.servers <span class="token operator">=</span> node01:9092,node02:9092,node03:9092
a1.sinks.k1.kafka.flumeBatchSize <span class="token operator">=</span> <span class="token number">20</span>
a1.sinks.k1.kafka.producer.acks <span class="token operator">=</span> <span class="token number">1</span>
</code></pre></div><p>2、创建topic</p> <div class="language-shell extra-class"><pre class="language-shell"><code>kafka-topics.sh --create --topic kaikeba --partitions <span class="token number">3</span> --replication-factor <span class="token number">2</span>  --zookeeper node01:2181,node02:2181,node03:2181
</code></pre></div><p>3、启动flume</p> <div class="language-shell extra-class"><pre class="language-shell"><code>bin/flume-ng agent -n a1 -c conf -f conf/flume-kafka.conf -Dflume.root.logger<span class="token operator">=</span>info,console
</code></pre></div><p>4、启动kafka控制台消费者，验证数据写入成功</p> <div class="language-shell extra-class"><pre class="language-shell"><code>kafka-console-consumer.sh --topic kaikeba --bootstrap-server node01:9092,node02:9092,node03:9092  --from-beginning
</code></pre></div><h2 id="kafka监控工具安装和使用"><a href="#kafka监控工具安装和使用" class="header-anchor">#</a> kafka监控工具安装和使用</h2> <h4 id="工具-kafka-manager"><a href="#工具-kafka-manager" class="header-anchor">#</a> 工具-Kafka Manager</h4> <p>kafkaManager它是由雅虎开源的可以监控整个kafka集群相关信息的一个工具。</p> <p>（1）可以管理几个不同的集群</p> <p>（2）监控集群的状态(topics, brokers, 副本分布, 分区分布)</p> <p>（3）创建topic、修改topic相关配置</p> <h6 id="_1、上传安装包到node01"><a href="#_1、上传安装包到node01" class="header-anchor">#</a> 1、上传安装包到node01</h6> <div class="language- extra-class"><pre class="language-text"><code>kafka-manager-1.3.0.4.zip
</code></pre></div><h6 id="_2、解压安装包"><a href="#_2、解压安装包" class="header-anchor">#</a> 2、解压安装包</h6> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token function">sudo</span> yum -y <span class="token function">install</span> <span class="token function">unzip</span>
<span class="token function">unzip</span> kafka-manager-1.3.0.4.zip  -d /kkb/install 
</code></pre></div><h6 id="_3、修改配置文件"><a href="#_3、修改配置文件" class="header-anchor">#</a> 3、修改配置文件</h6> <div class="language- extra-class"><pre class="language-text"><code>cd /kkb/install/kafka-manager-1.3.0.4/conf
vim application.conf
</code></pre></div><div class="language-shell extra-class"><pre class="language-shell"><code><span class="token comment">#修改kafka-manager.zkhosts的值，指定kafka集群地址</span>
kafka-manager.zkhosts<span class="token operator">=</span><span class="token string">&quot;node01:2181,node02:2181,node03:2181&quot;</span>
</code></pre></div><h6 id="_4、启动kafka-manager"><a href="#_4、启动kafka-manager" class="header-anchor">#</a> 4、启动kafka-manager</h6> <blockquote><p>启动zk集群，kafka集群，再使用root用户启动kafka-manager服务。</p> <p>注意：因为要使用root用户启动，所以，==建议在/etc/profile上重新配置以下Java的环境变量==，虽然在hadoop用户的.bash_profile上配置过了，但是启动kafka-manager时还是会报错：java没有找到。</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token function">su</span> - root
<span class="token function">vi</span> /etc/profile

<span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>xxx
<span class="token builtin class-name">export</span> <span class="token assign-left variable">Path</span><span class="token operator">=</span>xxx:<span class="token variable">$JAVA_HOME</span>/bin
</code></pre></div><p>前台启动：</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token builtin class-name">cd</span> /kkb/install/kafka-manager-1.3.0.4/
bin/kafka-manager -Dconfig.file<span class="token operator">=</span>conf/application.conf 
<span class="token comment">#-Dconfig.file=conf/application.conf 指定配置文件</span>
</code></pre></div><p>后台启动：</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token builtin class-name">cd</span> /kkb/install/kafka-manager-1.3.0.4/
<span class="token function">nohup</span> bin/kafka-manager -Dconfig.file<span class="token operator">=</span>conf/application.conf <span class="token operator">&amp;</span>
</code></pre></div><p>还可以通过-Dhttp.port手动指定kafka-manager的端口（默认端口是: 9000)</p> <div class="language-sh extra-class"><pre class="language-sh"><code>bin/kafka-manager -Dconfig.file<span class="token operator">=</span>conf/application.conf -Dhttp.port<span class="token operator">=</span><span class="token number">8080</span>
</code></pre></div></blockquote> <h6 id="_5、访问地址"><a href="#_5、访问地址" class="header-anchor">#</a> 5、访问地址</h6> <p>node01:9000</p> <p>通过web界面添加kafka 集群的方法：（kafka集群可以添加多个，也就是说该工具可以监控多个集群）</p> <ul><li>cluster name随便取一个名字即可，如MyKafka</li> <li>cluster zookeeper Hosts填我们的zookeeper集群地址</li> <li>Kafka version 随便选取一个即可</li></ul> <img src="kafka.assets/image-20200421174822032.png" alt="image-20200421174822032" style="zoom:80%;"> <p>添加好kafka集群后，查看自己的集群情况：</p> <p><img src="/assets/img/image-20200421175215928.7ba7e379.png" alt="image-20200421175215928"></p> <h4 id="工具-kafkaoffsetmonitor"><a href="#工具-kafkaoffsetmonitor" class="header-anchor">#</a> 工具-KafkaOffsetMonitor</h4> <p>该监控是基于一个jar包的形式运行，部署较为方便。==只有监控功能==，使用起来也较为安全</p> <p>(1)消费者组列表</p> <p>(2)查看topic的历史消费信息.</p> <p>(3)每个topic的所有parition列表(topic,pid,offset,logSize,lag,owner)</p> <p>(4)对consumer消费情况进行监控,并能列出每个consumer offset,滞后数据。</p> <h6 id="_1、下载安装包"><a href="#_1、下载安装包" class="header-anchor">#</a> 1、下载安装包</h6> <div class="language- extra-class"><pre class="language-text"><code>KafkaOffsetMonitor-assembly-0.2.0.jar
</code></pre></div><h6 id="_2、在服务器上新建一个目录kafka-moitor-把jar包上传到该目录中"><a href="#_2、在服务器上新建一个目录kafka-moitor-把jar包上传到该目录中" class="header-anchor">#</a> 2、在服务器上新建一个目录kafka_moitor，把jar包上传到该目录中</h6> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token punctuation">[</span>hadoop@node01 soft<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> /kkb/install/
<span class="token punctuation">[</span>hadoop@node01 install<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> kafka_moitor
<span class="token punctuation">[</span>hadoop@node01 install<span class="token punctuation">]</span>$ <span class="token function">mv</span> /kkb/soft/KafkaOffsetMonitor-assembly-0.2.0.jar /kkb/install/kafka_moitor/
</code></pre></div><h6 id="_3、在kafka-moitor目录下新建一个脚本"><a href="#_3、在kafka-moitor目录下新建一个脚本" class="header-anchor">#</a> 3、在kafka_moitor目录下新建一个脚本</h6> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token builtin class-name">cd</span> /kkb/install/kafka_moitor/
<span class="token function">vim</span> start_kafka_web.sh
</code></pre></div><div class="language-shell extra-class"><pre class="language-shell"><code><span class="token shebang important">#!/bin/sh</span>
java -cp KafkaOffsetMonitor-assembly-0.2.0.jar com.quantifind.kafka.offsetapp.OffsetGetterWeb --zk node01:2181,node02:2181,node03:2181 --port <span class="token number">8089</span> --refresh <span class="token number">10</span>.seconds --retain <span class="token number">1</span>.days
</code></pre></div><h6 id="_4、启动脚本"><a href="#_4、启动脚本" class="header-anchor">#</a> 4、启动脚本</h6> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token function">nohup</span> <span class="token function">sh</span> start_kafka_web.sh <span class="token operator">&amp;</span>
</code></pre></div><h6 id="_5、访问地址-2"><a href="#_5、访问地址-2" class="header-anchor">#</a> 5、访问地址</h6> <p>在浏览器中即可使用node01:8089访问kafka的监控页面。</p> <p><img src="/assets/img/1577435232773.7ca5282d.png" alt="1577435232773"></p> <p><img src="/assets/img/1577435264270.6f09dca8.png" alt="1577435264270"></p> <h4 id="工具-kafka-eagle-推荐"><a href="#工具-kafka-eagle-推荐" class="header-anchor">#</a> 工具-Kafka Eagle(推荐)</h4> <p>1、下载Kafka Eagle安装包</p> <ul><li>http://download.smartloli.org/
<ul><li>kafka-eagle-bin-1.2.3.tar.gz</li></ul></li></ul> <p>2、解压</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token function">tar</span> -zxvf /kkb/soft/kafka-eagle-bin-1.2.3.tar.gz -C /kkb/install/

<span class="token builtin class-name">cd</span> /kkb/install/kafka-eagle-bin-1.2.3/
<span class="token function">tar</span> -zxvf kafka-eagle-web-1.2.3-bin.tar.gz 

<span class="token function">mv</span> kafka-eagle-web-1.2.3 kafka-eagle-web
</code></pre></div><p>3、修改配置文件</p> <p>进入到conf目录, 修改system-config.properties</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token builtin class-name">cd</span> /kkb/install/kafka-eagle-bin-1.2.3/kafka-eagle-web/conf

<span class="token function">vi</span> system-config.properties
</code></pre></div><div class="language-sh extra-class"><pre class="language-sh"><code><span class="token comment">## 填上你的kafka集群信息</span>
kafka.eagle.zk.cluster.alias<span class="token operator">=</span>cluster1  <span class="token comment">#别名</span>
cluster1.zk.list<span class="token operator">=</span>node01:2181,node02:2181,node03:2181

<span class="token comment">## kafka eagle页面访问端口</span>
kafka.eagle.webui.port<span class="token operator">=</span><span class="token number">8048</span>

<span class="token comment">## kafka sasl authenticate</span>
kafka.eagle.sasl.enable<span class="token operator">=</span>false
kafka.eagle.sasl.protocol<span class="token operator">=</span>SASL_PLAINTEXT
kafka.eagle.sasl.mechanism<span class="token operator">=</span>PLAIN
kafka.eagle.sasl.client<span class="token operator">=</span>/kkb/install/kafka-eagle-bin-1.2.3/kafka-eagle-web/conf/kafka_client_jaas.conf  <span class="token comment">#这个要自己添加</span>

<span class="token comment">##  添加刚刚导入的ke数据库配置，我这里使用的是mysql</span>
kafka.eagle.driver<span class="token operator">=</span>com.mysql.jdbc.Driver
kafka.eagle.url<span class="token operator">=</span>jdbc:mysql://node03:3306/ke?useUnicode<span class="token operator">=</span>true<span class="token operator">&amp;</span><span class="token assign-left variable">characterEncoding</span><span class="token operator">=</span>UTF-8<span class="token operator">&amp;</span><span class="token assign-left variable">zeroDateTimeBehavior</span><span class="token operator">=</span>convertToNull
kafka.eagle.username<span class="token operator">=</span>root
kafka.eagle.password<span class="token operator">=</span><span class="token number">123456</span>
</code></pre></div><p>4、配置环境变量</p> <p>vi /etc/profile</p> <div class="language- extra-class"><pre class="language-text"><code>export KE_HOME=/kkb/install/kafka-eagle-bin-1.2.3/kafka-eagle-web
export PATH=$PATH:$KE_HOME/bin

source /etc/profile
</code></pre></div><p>5、启动kafka-eagle</p> <p>进入到$KE_HOME/bin目录，执行脚本sh ke.sh start</p> <div class="language-sh extra-class"><pre class="language-sh"><code><span class="token punctuation">[</span>hadoop@node01 conf<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> <span class="token variable">$KE_HOME</span>/bin
<span class="token punctuation">[</span>hadoop@node01 bin<span class="token punctuation">]</span>$ <span class="token function">sh</span> ke.sh start
</code></pre></div><p>6、访问地址</p> <p>启动成功后在浏览器中输入http://node01:8048/ke就可以访问kafka eagle 了。</p> <video src="E:\LearningAll\8-HadoopEcosystem-Video\kafka.assets\kafka-eagle.mp4"></video>
## kafka内核原理
<h4 id="isr机制"><a href="#isr机制" class="header-anchor">#</a> ISR机制</h4> <p>光是依靠多副本机制能保证Kafka的高可用性，但是能保证数据不丢失吗？</p> <p>不行，因为如果leader宕机，但是leader的数据还没同步到follower上去，此时即使选举了follower作为新的leader，当时刚才的数据已经丢失了。</p> <p>ISR是：in-sync replica，就是跟leader partition保持同步的follower partition的数量，只有处于ISR列表中的follower才可以在leader宕机之后被选举为新的leader，因为在这个ISR列表里代表他的数据跟leader是同步的。</p> <p>如果要保证写入kafka的数据不丢失，首先需要保证ISR中至少有一个follower，其次就是在一条数据写入了leader partition之后，要求必须复制给ISR中所有的follower partition，才能说代表这条数据已提交，绝对不会丢失，这是Kafka给出的承诺。</p> <p>leader partition挂掉时，如果ISR列表里没有follower partition，将会报异常。</p> <h4 id="hw-leo原理"><a href="#hw-leo原理" class="header-anchor">#</a> HW&amp;LEO原理</h4> <p>LEO</p> <blockquote><p>last end offset，日志末端偏移量，==标识当前日志文件中下一条待写入的消息的offset==。举一个例子，若LEO=10，那么表示在该副本日志上已经保存了10条消息，位移范围是[0，9]。</p></blockquote> <p>HW</p> <blockquote><p>Highwatermark，俗称==高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息==。任何一个副本对象的HW值一定不大于其LEO值。
小于或等于HW值的所有消息被认为是“已提交的”或“已备份的”。==HW它的作用主要是用来判断副本的备份进度.==
下图表示一个日志文件，这个日志文件中只有9条消息，第一条消息的offset（LogStartOffset）为0，最后一条消息的offset为8，offset为9的消息使用虚线表示的，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取offset在 0 到 5 之间的消息，offset为6的消息对消费者而言是不可见的。</p></blockquote> <p><img src="/assets/img/691225277.7d0befa0.png" alt="img"></p> <p><img src="/assets/img/u=1118880152,1351290688&amp;fm=26&amp;gp=0.28371af0.jpg" alt="img"></p> <blockquote><p>==leader持有的HW即为分区的HW,同时leader所在broker还保存了所有follower副本的leo==</p> <p>（1）关系：leader的leo &gt;= follower的leo &gt;= leader保存的follower的leo &gt;= leader的hw &gt;= follower的hw</p> <p>（2）原理：上面关系反应出各个值的更新逻辑的先后</p></blockquote> <h4 id="更新leo的机制"><a href="#更新leo的机制" class="header-anchor">#</a> 更新LEO的机制</h4> <blockquote><p>注意:==follower副本的LEO保存在2个地方==</p> <p>（1）follower副本所在的broker缓存里。</p> <p>（2）leader所在broker的缓存里，也就是leader所在broker的缓存上保存了该分区所有副本的LEO。</p> <p>更新LEO的时机</p> <p>==follower更新LEO==</p> <p>（1）follower的leo更新时间: 每当follower副本写入一条消息时，leo值会被更新</p> <p>（2）leader端的follower副本的leo更新时间: 当follower从leader处fetch消息时，leader获取follower的fetch请求中的offset参数，更新保存在leader端follower的leo。</p> <p>==leader更新LEO==</p> <p>（1）leader本身的leo的更新时间：leader向log写消息时</p></blockquote> <h4 id="更新hw的机制"><a href="#更新hw的机制" class="header-anchor">#</a> 更新HW的机制</h4> <blockquote><p>==1、follower更新HW==</p> <p>follower更新HW发生在其更新完LEO后，即==follower向log写完数据，它就会尝试更新HW值==。具体算法就是比较当前LEO(已更新)与fetch响应中leader的HW值，取两者的小者作为新的HW值，即 min(leader HW, follower leo)。</p> <p>==2、leader更新HW的时机==</p> <p>（1）producer 向 leader 写消息时</p> <p>（2）leader 处理 follower 的 fetch 请求时</p> <p>（3）某副本成为leader时</p> <p>（4）broker 崩溃导致副本被踢出ISR时</p> <p>==3、leader更新HW的方式==</p> <p>当尝试确定分区HW时，它会选出所有满足条件的副本，比较它们的LEO（当然也包括leader自己的LEO），并==选择最小的LEO值作为HW值==,leader HW=min(所有follower leo) 。</p> <p>这里的满足条件主要是指副本要满足以下两个条件之一：</p> <p>（1）处于ISR中</p> <p>（2）副本LEO落后于leader LEO的时长不大于replica.lag.time.max.ms参数值（默认值是10秒），或者落后Leader的条数超过预定值replica.lag.max.messages（默认值是4000）。也就是说如果某follower在10s秒内没有向leader同步数据，就会被踢出ISR列表，或者落后leader4000条数据时也会踢出ISR列表。</p> <p><img src="/assets/img/HW和LEO的更新.762367f9.png" alt="HW和LEO的更新"></p></blockquote> <h2 id="producer消息发送原理"><a href="#producer消息发送原理" class="header-anchor">#</a> producer消息发送原理</h2> <h6 id="producer核心流程概览"><a href="#producer核心流程概览" class="header-anchor">#</a> producer核心流程概览</h6> <p><img src="/assets/img/Producer流程分析.7cbf6d24.png" alt="Producer流程分析"></p> <ul><li><p>1、构建KafkaProducer对象，构建后会初始化一个ProducerInterceptors，它是一个拦截器，对发送的数据进行拦截</p> <div class="language- extra-class"><pre class="language-text"><code>ps：说实话这个功能其实没啥用，我们即使真的要过滤，拦截一些消息，也不考虑使用它，我们直接发送数据之前自己用代码过滤即可
</code></pre></div></li> <li><p>2、Serializer 对消息的key和value进行序列化</p></li> <li><p>3、通过使用分区器作用在每一条消息上，实现数据分发进行入到topic不同的分区中</p></li> <li><p>4、RecordAccumulator收集消息，实现批量发送</p> <div class="language- extra-class"><pre class="language-text"><code>它是一个缓冲区，可以缓存一批数据，把topic的每一个分区数据存在一个队列Deque中，然后封装消息成一个一个的batch批次，每一个batch默认是16KB，最后实现数据分批次批量发送。缓冲区大小默认是32M。
缓冲区就是生产者的Java开发代码中的props.put(&quot;buffer.memory&quot;,xxx)
batch批次就是生产者的Java开发代码中的props.put(&quot;batch.size&quot;,xxx)
</code></pre></div></li> <li><p>5、Sender线程从RecordAccumulator获取消息</p></li> <li><p>6、构建ClientRequest对象</p></li> <li><p>7、将ClientRequest交给 NetWorkClient准备发送</p></li> <li><p>8、NetWorkClient 将通过RPC方式将请求放入到KafkaChannel的缓存</p></li> <li><p>9、发送请求到kafka集群</p></li> <li><p>10、调用回调函数，接受到响应</p></li></ul> <h2 id="producer核心参数"><a href="#producer核心参数" class="header-anchor">#</a> producer核心参数</h2> <h4 id="回顾之前的producer生产者代码"><a href="#回顾之前的producer生产者代码" class="header-anchor">#</a> 回顾之前的producer生产者代码</h4> <div class="language-java extra-class"><pre class="language-java"><code><span class="token keyword">package</span> <span class="token namespace">com<span class="token punctuation">.</span>kaikeba<span class="token punctuation">.</span>producer</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token operator">*</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>concurrent<span class="token punctuation">.</span></span><span class="token class-name">ExecutionException</span><span class="token punctuation">;</span>

<span class="token comment">/**
 * 需求：开发kafka生产者代码
 */</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">KafkaProducerStudyDemo</span> <span class="token punctuation">{</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">ExecutionException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">{</span>
        <span class="token comment">//准备配置属性</span>
        <span class="token class-name">Properties</span> props <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//kafka集群地址</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;bootstrap.servers&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;node01:9092,node02:9092,node03:9092&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//acks它代表消息确认机制   // 1 0 -1 all</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;acks&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;all&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//重试的次数</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;retries&quot;</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//批处理数据的大小，每次写入多少数据到topic</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;batch.size&quot;</span><span class="token punctuation">,</span> <span class="token number">16384</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//可以延长多久发送数据</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;linger.ms&quot;</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//缓冲区的大小</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;buffer.memory&quot;</span><span class="token punctuation">,</span> <span class="token number">33554432</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;key.serializer&quot;</span><span class="token punctuation">,</span>
                  <span class="token string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;value.serializer&quot;</span><span class="token punctuation">,</span>
                  <span class="token string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//添加自定义分区函数</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">&quot;partitioner.class&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;com.kaikeba.partitioner.MyPartitioner&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token class-name">Producer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> producer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaProducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>props<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>

            <span class="token comment">// 这是异步发送的模式</span>
            producer<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token string">&quot;test&quot;</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&quot;hello-kafka-&quot;</span><span class="token operator">+</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Callback</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
                <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onCompletion</span><span class="token punctuation">(</span><span class="token class-name">RecordMetadata</span> metadata<span class="token punctuation">,</span> <span class="token class-name">Exception</span> exception<span class="token punctuation">)</span> <span class="token punctuation">{</span>
                    <span class="token keyword">if</span><span class="token punctuation">(</span>exception <span class="token operator">==</span> <span class="token keyword">null</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
                        <span class="token comment">// 消息发送成功</span>
                        <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;消息发送成功&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
                        <span class="token comment">// 消息发送失败，需要重新发送</span>
                    <span class="token punctuation">}</span>
                <span class="token punctuation">}</span>

            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token comment">// 这是同步发送的模式</span>
            <span class="token comment">//producer.send(record).get();</span>
            <span class="token comment">// 你要一直等待人家后续一系列的步骤都做完，发送消息之后</span>
            <span class="token comment">// 有了消息的回应返回给你，你这个方法才会退出来</span>
        <span class="token punctuation">}</span>
        producer<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

<span class="token punctuation">}</span>
</code></pre></div><h4 id="参数之常见异常处理"><a href="#参数之常见异常处理" class="header-anchor">#</a> 参数之常见异常处理</h4> <p>观察上面的生产者代码，发送数据是可以同步或者异步的，但不管是异步还是同步，都可能让你处理异常，常见的异常如下：</p> <blockquote><p>1）LeaderNotAvailableException：这个就是如果某台机器挂了，此时leader副本不可用，会导致你写入失败，要等待其他follower副本切换为leader副本之后，才能继续写入，此时可以==重试发送即可==。如果说你平时重启kafka的broker进程，肯定会导致leader切换，一定会导致你写入报错：LeaderNotAvailableException</p> <p>2）NotControllerException：这个也是同理，如果说Controller所在Broker挂了，那么此时会有问题，需要等待Controller重新选举，此时==也是一样就是重试即可==</p> <p>3）NetworkException：网络异常，==重试即可==
我们之前配置了一个参数，retries，他会自动重试的，但是如果重试几次之后还是不行，就会提供Exception给我们来处理了。</p></blockquote> <p>下面我们来看一下重试的相关参数：</p> <blockquote><p>(1)retries</p> <p>重新发送数据的次数，默认为0，表示不重试，</p> <p>==在重试次数内切换leader副本成功/Controller重新选举成功/网络恢复正常，就不会报告上面的异常，程序继续运行。==</p> <p>(2)retry.backoff.ms</p> <p>两次重试之间的时间间隔，默认为100ms</p></blockquote> <h4 id="参数之提升消息吞吐量"><a href="#参数之提升消息吞吐量" class="header-anchor">#</a> 参数之提升消息吞吐量</h4> <p>吞吐量可以理解为单位时间内发送的数据量。可以通过下列这些参数来提高消息的吞吐量。</p> <blockquote><p>（1）buffer.memory</p> <p>设置发送消息的缓冲区，默认值是33554432，就是32MB</p> <p>如果发送消息出去的速度小于写入消息进去的速度，就会导致缓冲区写满，此时生产消息就会阻塞住，所以说==这里就应该多做一些压测，尽可能保证说这块缓冲区不会被写满导致生产行为被阻塞住==</p> <p>（2）compression.type</p> <p>producer用于压缩数据的压缩类型。默认是none表示无压缩。可以指定gzip、snappy</p> <p>==压缩最好用于批量处理，批量处理消息越多，压缩性能越好。==</p> <p>（3）batch.size</p> <p>producer将试图批处理消息记录，以==减少请求次数==。这将改善client与server之间的性能。</p> <p>默认是16384Bytes，即16kB，也就是一个batch满了16kB就发送出去</p> <p>==如果batch太小，会导致频繁网络请求，吞吐量下降；如果batch太大，会导致一条消息需要等待很久才能被发送出去，而且会让内存缓冲区有很大压力，过多数据缓冲在内存里。==</p> <p>（4）linger.ms</p> <p>这个值默认是0，就是消息必须立即被发送</p> <p>一般设置一个100毫秒之类的，这样的话就是说，这个消息被发送出去后进入一个batch，如果100毫秒内，这个batch满了16kB，自然就会发送出去。但是如果100毫秒内，batch没满，那么也必须把消息发送出去了，不能让消息的发送延迟时间太长，也避免给内存造成过大的一个压力。</p></blockquote> <h4 id="参数之请求超时"><a href="#参数之请求超时" class="header-anchor">#</a> 参数之请求超时</h4> <blockquote><p>(1)max.request.size</p> <ul><li>这个参数用来==控制发送出去的一条消息的大小==，默认是1048576字节，也就1mb</li> <li>这个一般太小了，很多消息可能都会超过1mb的大小，所以需要自己优化调整，把他设置更大一些（企业一般设置成10M）</li></ul> <p>(2)request.timeout.ms</p> <ul><li>这个就是说发送一个请求出去之后，他有一个超时的时间限制，默认是30秒</li> <li>如果==30秒都收不到响应，那么就会认为异常，会抛出一个TimeoutException来让我们进行处理==</li></ul></blockquote> <h4 id="ack参数"><a href="#ack参数" class="header-anchor">#</a> ACK参数</h4> <p>acks参数，其实是控制发送出去的消息的持久化机制的。ack参数的有如下的值：</p> <blockquote><p>(1)acks=0</p> <p>==生产者只管发数据，不管消息是否写入成功到broker中，数据丢失的风险最高==</p> <p>producer根本不管写入broker的消息到底成功没有，发送一条消息出去，立马就可以发送下一条消息，这是吞吐量最高的方式，但是可能消息都丢失了。你也不知道的，但是说实话，你如果真是那种实时数据流分析的业务和场景，就是仅仅分析一些数据报表，丢几条数据影响不大的。会让你的发送吞吐量会提升很多，你发送弄一个batch出去，不需要等待人家leader写成功，直接就可以发送下一个batch了，吞吐量很大的，哪怕是偶尔丢一点点数据，实时报表，折线图，饼图。</p> <p>（2）acks=1</p> <p>==只要leader写入成功，就认为消息成功了==。默认给这个其实就比较合适的，还是可能会导致数据丢失的，如果刚写入leader，leader就挂了，此时数据必然丢了，其他的follower没收到数据副本，变成leader.</p> <p>（2）acks=all 或者 acks=-1</p> <p>这个leader写入成功以后，==必须等待其他ISR中的副本都写入成功，才可以返回响应说这条消息写入成功了==，此时你会收到一个回调通知==》“这种方式数据最安全，但是性能最差”。</p></blockquote> <p>如果要想保证数据不丢失，得如下设置：</p> <blockquote><p>（1）min.insync.replicas = 2</p> <p>ISR里必须有2个副本，一个leader和一个follower，最最起码的一个，不能只有一个leader存活，连一个follower都没有。</p> <p>（2）acks = -1</p> <p>每次写成功一定是leader和follower都成功才可以算做成功，这样leader挂了，follower上是一定有这条数据，不会丢失。</p> <p>（3）retries = Integer.MAX_VALUE</p> <p>无限重试，如果上述两个条件不满足，写入一直失败，就会无限次重试，保证说数据必须成功的发送给两个副本，如果做不到，就不停的重试。</p> <p>==除非是面向金融级的场景，面向企业大客户，或者是广告计费，跟钱的计算相关的场景下，才会通过严格配置保证数据绝对不丢失==</p></blockquote> <h4 id="重试乱序"><a href="#重试乱序" class="header-anchor">#</a> 重试乱序</h4> <blockquote><p>max.in.flight.requests.per.connection，表示==每个网络连接已经发送但还没有收到服务端响应的请求个数最大值。==</p> <p>==消息重试是可能导致消息的乱序的，因为可能排在你后面的消息都发送出去了，你现在收到回调失败了才在重试，此时消息就会乱序==。</p> <p>所以可以使用“max.in.flight.requests.per.connection”==参数设置为1，这样可以保证producer必须把一个请求发送的数据发送成功了再发送后面的请求。避免数据出现乱序==</p> <img src="kafka.assets/image-20200422040917064.png" alt="image-20200422040917064" style="zoom:47%;"></blockquote> <h2 id="broker核心参数"><a href="#broker核心参数" class="header-anchor">#</a> broker核心参数</h2> <p>server.properties配置文件核心参数</p> <blockquote><p>【broker.id】</p> <p>每个broker都必须自己设置的一个唯一id</p> <p>【log.dirs】</p> <p>这个极为重要，kafka的所有数据就是写入这个目录下的磁盘文件中的，==如果说机器上有多块物理硬盘，那么可以把多个目录挂载到不同的物理硬盘上，然后这里可以设置多个目录，这样kafka可以数据分散到多块物理硬盘，多个硬盘的磁头可以并行写，这样可以提升吞吐量。==</p> <p>【zookeeper.connect】</p> <p>连接kafka底层的zookeeper集群的</p> <p>【Listeners】</p> <p>broker监听客户端发起请求的端口号，默认是9092</p> <p>【unclean.leader.election.enable】</p> <p>默认是false，意思就是只能选举ISR列表里的follower成为新的leader，1.0版本后才设为false，之前都是true，允许非ISR列表的follower选举为新的leader</p> <p>【delete.topic.enable】</p> <p>默认true，允许删除topic</p> <p>【log.retention.hours】</p> <p>可以设置一下，要保留数据多少个小时(默认168小时)，这个就是底层的磁盘文件，默认保留7天的数据，根据自己的需求来就行了</p></blockquote> <h2 id="consumer消费原理"><a href="#consumer消费原理" class="header-anchor">#</a> consumer消费原理</h2> <h4 id="offset管理"><a href="#offset管理" class="header-anchor">#</a> Offset管理</h4> <blockquote><p>每个consumer内存里数据结构保存对每个topic的每个分区的消费offset，定期会提交offset，老版本是写入zk，但是那样高并发请求zk是不合理的架构设计，zk是做分布式系统的协调的，轻量级的元数据存储，不能负责高并发读写，作为数据存储。</p> <p>所以后来就是提交offset发送给内部topic：==consumer_offsets，提交过去的时候，key是group.id+topic+分区号，value就是当前offset的值，每隔一段时间，kafka内部会对这个topic进行compact。也就是每个group.id+topic+分区号就保留最新的那条数据即可。==</p> <p>而且因为这个 ==__consumer_offsets可能会接收高并发的请求，所以默认分区50个，这样如果你的kafka部署了一个大的集群，比如有50台机器，就可以用50台机器来抗offset提交的请求压力==，就好很多。</p></blockquote> <h4 id="coordinator"><a href="#coordinator" class="header-anchor">#</a> Coordinator</h4> <p>Coordinator的作用</p> <blockquote><p>每个consumer group都会选择一个broker作为自己的coordinator，他是负责监控这个消费组里的各个消费者的心跳，以及判断是否宕机，然后开启rebalance.</p> <p>根据内部的一个选择机制，会挑选一个对应的Broker，==Kafka总会把你的各个消费组均匀分配给各个Broker作为coordinator来进行管理的.==</p> <p>==consumer group中的每个consumer刚刚启动就会跟选举出来的这个consumer group对应的coordinator所在的broker进行通信，然后由coordinator分配分区给你的这个consumer来进行消费==。coordinator会尽可能均匀的分配分区给各个consumer来消费。</p></blockquote> <p>如何选择哪台是coordinator</p> <blockquote><p>==首先对消费组的groupId进行hash，接着对consumer_offsets的分区数量取模==，默认是50，可以通过offsets.topic.num.partitions来设置，找到你的这个consumer group的offset要提交到consumer_offsets的哪个分区。</p> <p>比如说：groupId，&quot;membership-consumer-group&quot; -&gt; hash值（数字）-&gt; 对50取模 -&gt; 就知道这个consumer group下的所有的消费者提交offset的时候是往哪个分区去提交offset。</p> <p>找到consumer_offsets的一个分区，consumer_offset的分区的副本数量默认来说1，只有一个leader，然后对这个分区找到对应的leader所在的broker，这个broker就是这个consumer group的coordinator了，consumer接着就会维护一个Socket连接跟这个Broker进行通信。</p></blockquote> <h4 id="消费流程"><a href="#消费流程" class="header-anchor">#</a> 消费流程</h4> <p><img src="/assets/img/GroupCoordinator原理剖析.04d85811.png" alt="39 GroupCoordinator原理剖析"></p> <ol><li>对group.id进行hash运算，取模，得到offset要提交到consumer_offsets的哪个分区，找到这个分区对应的leader，该leader所在的broker就是这个消费组的的coordinator了。</li> <li>选出coordinator后，消费组的所有消费者向coordinator发送注册请求，最先发送请求的消费者将称为consumer（leader）。</li> <li>consumer（leader）指定消费方案，并发送给coordinator。</li> <li>coordinator收到方案后，下发消费方案到该消费者所有的consumer。</li> <li>每个consumer根据方案，找到对应的leader分区，开始消费数据。</li></ol> <h2 id="consumer消费者rebalance策略"><a href="#consumer消费者rebalance策略" class="header-anchor">#</a> consumer消费者Rebalance策略</h2> <blockquote><p>比如我们消费的一个topic主题有==12个分区==：p0,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11</p> <p>假设我们的消费者组里面有==三个消费者==。</p></blockquote> <h4 id="range范围策略"><a href="#range范围策略" class="header-anchor">#</a> range范围策略</h4> <p>RangeAssignor（默认策略）</p> <blockquote><p>range策略就是按照partiton的序号范围
p0~3             consumer1
p4~7             consumer2
p8~11            consumer3</p> <p>默认就是这个策略</p></blockquote> <h4 id="round-robin轮训策略"><a href="#round-robin轮训策略" class="header-anchor">#</a> round-robin轮训策略</h4> <p>RoundRobinAssignor</p> <blockquote><p>consumer1:	0,3,6,9</p> <p>consumer2:	1,4,7,10</p> <p>consumer3:	2,5,8,11</p> <p>但是==前面的这两个方案有个问题：==</p> <p>假设consuemr1挂了，会重新指定方案，p0-5分配给consumer2,p6-11分配给consumer3</p> <p>这样的话，原本在consumer2上的的p6,p7分区就被分配到了 consumer3上</p></blockquote> <h4 id="sticky黏性策略"><a href="#sticky黏性策略" class="header-anchor">#</a> sticky黏性策略</h4> <p>StickyAssignor</p> <blockquote><p>最新的一个sticky策略，就是说尽可能保证在rebalance的时候，==让原本属于这个consumer
的分区还是属于他们，然后把多余的分区再均匀分配过去==，这样尽可能维持原来的分区分配的策略</p> <p>consumer1： 0-3</p> <p>consumer2:  4-7</p> <p>consumer3:  8-11</p> <p>假设consumer3挂了</p> <p>consumer1：0-3，+8,9</p> <p>consumer2: 4-7，+10,11</p></blockquote> <h4 id="消费者分配策略的控制"><a href="#消费者分配策略的控制" class="header-anchor">#</a> 消费者分配策略的控制</h4> <p>由参数==partition.assignment.strategy==控制，默认是RangeAssignor表示范围策略</p> <div class="language-java extra-class"><pre class="language-java"><code><span class="token comment">//设置消费者分配策略：</span>
properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>PARTITION_ASSIGNMENT_STRATEGY_CONFIG<span class="token punctuation">,</span> 
<span class="token class-name">StickyAssignor</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><h2 id="consumer核心参数"><a href="#consumer核心参数" class="header-anchor">#</a> consumer核心参数</h2> <p>官网查看kafka参数<a href="http://kafka.apache.org/10/documentation.html" target="_blank" rel="noopener noreferrer">http://kafka.apache.org/10/documentation.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <blockquote><h6 id="【heartbeat-interval-ms】"><a href="#【heartbeat-interval-ms】" class="header-anchor">#</a> 【heartbeat.interval.ms】</h6> <p>默认值：3000</p> <p>consumer心跳时间，==必须得保持心跳才能知道consumer是否故障了==，然后如果故障之后，就会通过心跳</p> <p>下发rebalance的指令给其他的consumer通知他们进行rebalance的操作。</p> <p>leader会重新指定消费方案，然后提交给coordinator,再次下发消费方案给各个consumer</p> <h6 id="【session-timeout-ms】"><a href="#【session-timeout-ms】" class="header-anchor">#</a> 【session.timeout.ms】</h6> <p>默认值：10000</p> <p>kafka==多长时间感知不到一个consumer就认为他故障了==，默认是10秒</p> <h6 id="【max-poll-interval-ms】"><a href="#【max-poll-interval-ms】" class="header-anchor">#</a> 【max.poll.interval.ms】</h6> <p>默认值：300000</p> <p>如果在==两次poll操作之间，超过了这个时间，那么就会认为这个consume处理能力太弱了，会被踢出消费组==，分区分配给别人去消费，一遍来说结合你自己的业务处理的性能来设置就可以了</p> <h6 id="【fetch-max-bytes】"><a href="#【fetch-max-bytes】" class="header-anchor">#</a> 【fetch.max.bytes】</h6> <p>默认值：1048576=&gt;1M</p> <p>获取一==条消息最大的字节数==，一般建议设置大一些</p> <h6 id="【max-poll-records】"><a href="#【max-poll-records】" class="header-anchor">#</a> 【max.poll.records】</h6> <p>默认值：500条</p> <p>一次poll返回消息的==最大条数==，</p> <h6 id="【connections-max-idle-ms】"><a href="#【connections-max-idle-ms】" class="header-anchor">#</a> 【connections.max.idle.ms】</h6> <p>默认值：540000</p> <p>==consumer跟broker的socket连接如果空闲超过了一定的时间，此时就会自动回收连接==，但是下次消费就要重新建立socket连接，这个==建议设置为-1，不要去回收==</p> <h6 id="【auto-offset-reset】"><a href="#【auto-offset-reset】" class="header-anchor">#</a> 【auto.offset.reset】</h6> <p>earliest：当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费</p> <p>latest：当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从当前位置开始消费</p> <p>none：topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</p> <p>注：我们==生产里面一般设置的是latest==</p> <h6 id="【enable-auto-commit】"><a href="#【enable-auto-commit】" class="header-anchor">#</a> 【enable.auto.commit】</h6> <p>默认值：true</p> <p>设置为自动提交offset</p> <h6 id="【auto-commit-interval-ms】"><a href="#【auto-commit-interval-ms】" class="header-anchor">#</a> 【auto.commit.interval.ms】</h6> <p>默认值：60 * 1000</p> <p>==每隔多久更新一下偏移量==</p></blockquote> <h2 id="面试题"><a href="#面试题" class="header-anchor">#</a> 面试题</h2> <p><img src="/assets/img/image-20200422140129387.8655cc60.png" alt="image-20200422140129387"></p> <blockquote><p>如果消费者这端要保证数据被处理且只被处理一次，屏蔽掉下面这2种情况：
（1）数据的重复处理
（2）数据的丢失</p> <p>一般来说：==需要手动提交偏移量，需要保证数据处理成功与保存偏移量的操作在同一事务中就可以了==</p></blockquote> <h2 id="kafka优秀书籍推荐"><a href="#kafka优秀书籍推荐" class="header-anchor">#</a> kafka优秀书籍推荐</h2> <p><img src="/assets/img/1568440708295.56cdfaf0.png" alt="1568440708295"></p> <p><img src="/assets/img/1568440766949.6dd39e45.png" alt="1568440766949"></p> <h2 id="作业-kafka性能测试"><a href="#作业-kafka性能测试" class="header-anchor">#</a> 作业--kafka性能测试</h2> <p>博客地址: https://www.cnblogs.com/xiaodf/p/6023531.html</p></div> <footer class="page-edit"><div class="edit-link"><a href="https://github.com/MaLunan/press/edit/dev-mln/docs/kafka/kafka.md" target="_blank" rel="noopener noreferrer">在GitHub 上编辑此页！</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">1/19/2021, 1:41:59 AM</span></div></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.fdd7a502.js" defer></script><script src="/assets/js/16.375b747e.js" defer></script><script src="/assets/js/20.b3582e09.js" defer></script>
  </body>
</html>
